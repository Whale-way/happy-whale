{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Whale and Dolphin Classification Project"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Authors:\n",
    "- Victor Möslein\n",
    "- Maren Rieker\n",
    "- Reed Garvin\n",
    "- Dinah Rabe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This Notebook is one of three core notebooks of the Whale and Dolphin Classification Project for the \"Machine Learning\" class at the Hertie School. It focuses on the application of classic machine learning models to the task at hand. There is one other notebook concerned with data preprocessing and another that focuses on the application of a deep learning model.\n",
    "\n",
    "The code of this notebook partly follows the chapter on Classification from the book \"Hands-on Machine Learning with Scikit-Learn, Keras, and Tensorflow\" by Aurélien Géron."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Setup: System settings and packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"1.0\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "import pandas as pd\n",
    "from numpy import savez_compressed\n",
    "import os\n",
    "import timeit\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import PIL\n",
    "\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# to make this notebook's output stable\n",
    "np.random.seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_data_switch_on = True # if the full data set should be used, this switch need to be set to true"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define paths to data and for output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# path to clean data folder\n",
    "ROOT_PATH_DATA = \"input/04_cleaned/\"\n",
    "\n",
    "# where to save figures\n",
    "ROOT_PATH_FIG = \"output/ml_models/01_figures\"\n",
    "os.makedirs(ROOT_PATH_FIG, exist_ok=True)\n",
    "\n",
    "# where to save output\n",
    "\n",
    "ROOT_OUTPUT = \"output/ml_models/\"\n",
    "OUTPUT_PATH_TRAIN_EVAL = os.path.join(ROOT_OUTPUT + \"02_training_set_evaluation\")\n",
    "OUTPUT_PATH_TEST_EVAL = os.path.join(ROOT_OUTPUT + \"03_test_set_evaluation\")\n",
    "OUTPUT_PATH_HYPPAR_TUN = os.path.join(ROOT_OUTPUT + \"04_hyperparamter_tuning\")\n",
    "OUTPUT_PATH_RUN_TIME = os.path.join(ROOT_OUTPUT + \"05_runtime_stats\")\n",
    "\n",
    "# function to save figures\n",
    "\n",
    "def save_fig(fig_id, save_path=ROOT_PATH_FIG, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(save_path, fig_id + \".\" + fig_extension)\n",
    "    print(\">... Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading and splitting training data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(ROOT_PATH_DATA + \"train/clean_train.csv\", sep = ';')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_df.head() # Checking for correct dimensions after every step. The data set contains 51033 images."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_full = labels_df[\"species\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_full.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_full.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pic_ids_full = labels_df[\"image\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pic_ids_full.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pic_ids_full.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load npz files ## has to be adjusted to the actual one\n",
    "img_data = np.load(\"input/04_cleaned/train/img_data_full_48.npz\")\n",
    "img_data_full = img_data[\"arr_0\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_data_full.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split into training and test set - 10.000 test set / 40.000 full training set\n",
    "# stratify = labels splits it proportionally to classes in the dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "img_data_train_full, img_data_test_no_PCA, labels_train_full, labels_test_no_PCA, pic_ids_train_full, pic_ids_test_no_PCA = train_test_split(img_data_full , labels_full, pic_ids_full, stratify=labels_full, test_size=0.2, random_state=42)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split into training and validation set - 30.000 training set / 10.000 validation set\n",
    "img_data_train_no_PCA, img_data_val_no_PCA, labels_train_no_PCA, labels_val_no_PCA, pic_ids_train_no_PCA, pic_ids_val_no_PCA = train_test_split(img_data_train_full , labels_train_full, pic_ids_train_full, train_size=30000, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Speed improvements through dimensionality reduction\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Performing Principal Component Analysis on the training data\n",
    "#### this had to be moved to the beginning of the project due to computation time restrictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_data_full.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# defining to keep 99% of the variance of the data\n",
    "pca = PCA(.95)\n",
    "img_data_full_red = pca.fit_transform(img_data_full)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# checking how many features are left\n",
    "img_data_full_red.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1466 features are left when keeping 99% of variance (still too many)\n",
    "### 502 features are left when keeping 95% of variance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save dataset after PCA\n",
    "\n",
    "def store_dict_red(image_df, target_pixel):\n",
    "    if full_data_switch_on == True:\n",
    "        savez_compressed(\"input/04_cleaned/train/img_data_full_red_\"+str(target_pixel)+\".npz\",image_df)\n",
    "        print(\"file successfully stored in: input/04_cleaned/train/img_data_full_red_\"+str(target_pixel)+\".npz\")\n",
    "    elif full_data_switch_on == False:\n",
    "        savez_compressed(\"input/04_cleaned/train/img_data_sample_red_\"+str(target_pixel)+\".npz\",image_df)\n",
    "        print(\"file successfully stored in: input/04_cleaned/train/img_data_sample_red_\"+str(target_pixel)+\".npz\")\n",
    "    else:\n",
    "        raise ValueError(\"Full data switch is not correctly defined\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Store reduced image file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "store_dict_red(img_data_full_red, target_pixel=48)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load reduced image file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load npz files\n",
    "img_data_red = np.load(\"input/04_cleaned/train/img_data_full_red_48.npz\")\n",
    "img_data_full_red = img_data_red[\"arr_0\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split into training and test set - 10.000 test set / 40.000 full training set\n",
    "# stratify = labels splits it proportionally to classes in the dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "img_data_train_full, img_data_test, labels_train_full, labels_test, pic_ids_train_full, pic_ids_test = train_test_split(img_data_full_red , labels_full, pic_ids_full, stratify=labels_full, test_size=0.2, random_state=42)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split into training and validation set - 30.000 training set / 10.000 validation set\n",
    "img_data_train, img_data_val, labels_train, labels_val, pic_ids_train, pic_ids_val = train_test_split(img_data_train_full , labels_train_full, pic_ids_train_full, train_size=30000, random_state=42)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking for correct dimensions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_data_train_full.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_data_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_data_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_data_val.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The distribution between training and validation set is similar.\n",
    "labels_val.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_train_full.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# So now we have: 30000 images in the training set, 10826 images in the validation set, 10207 images in the test set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementing baseline model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_clasf(classifier_x, img_data_train, labels_train):\n",
    "    # set name of classifier\n",
    "    classifier_name = classifier_x.__class__.__name__\n",
    "\n",
    "    # train model\n",
    "    print(\">... Starting training of\", classifier_name)\n",
    "    start_time = timeit.default_timer()\n",
    "    classifier_x.fit(img_data_train, labels_train)\n",
    "    time_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    print(\">... Classifier {} sucessfully trained in {} seconds.\".format(classifier_name, round(time_elapsed,3)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Running this on the Hertie Server takes around 2 hours with max_iter = 500 and 24 minutes with max_iter = 100 on full image data\n",
    "# After PCA, it runs in 96 seconds locally with max_iter = 500\n",
    "\n",
    "# We set solver = 'sag' because stochastic average gradient descent works better on large data sets\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier_LR = LogisticRegression(random_state=42, max_iter = 500, solver = 'sag')\n",
    "train_clasf(classifier_LR, img_data_train, labels_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating baseline model (\"compute metrics on train AND dev\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# store predictions of classifier\n",
    "pred_train = classifier_LR.predict(img_data_train)\n",
    "pred_val = classifier_LR.predict(img_data_val)\n",
    "\n",
    "# evaluate classifier and store metrics\n",
    "evaluation_scores = {\"Precision Score Train\": precision_score(labels_train, pred_train, average = \"macro\").round(3),\n",
    "                     \"Precision Score Validation\": precision_score(labels_val, pred_val, average = \"macro\").round(3),\n",
    "                     \"Recall Score Train\": recall_score(labels_train, pred_train, average = \"macro\").round(3),\n",
    "                     \"Recall Score Validation\": recall_score(labels_val, pred_val, average = \"macro\").round(3),\n",
    "                     \"F1 Score Train\": f1_score(labels_train, pred_train, average=\"macro\").round(3),\n",
    "                     \"F1 Score Validation\": f1_score(labels_val, pred_val, average=\"macro\").round(3)}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save evaluation scores\n",
    "def store_eval_score(image_df, classifier_name):\n",
    "    savez_compressed(OUTPUT_PATH_TRAIN_EVAL + \"/evaluation_scores_\"+str(classifier_name)+\".npz\",image_df)\n",
    "    print(\"file successfully stored in: output/ml_models/02_training_set_evaluation\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "store_eval_score(evaluation_scores, \"LogisticRegression\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Multiclass Confusion Matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "conf_matrix = multilabel_confusion_matrix(labels_val,pred_val,labels=labels_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "disp_conf_matrix = ConfusionMatrixDisplay.from_predictions(labels_val, pred_val)\n",
    "\n",
    "disp_conf_matrix.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def function for saving the filtered error table\n",
    "\n",
    "def store_conf_matrix(disp_conf_matrix, classifier_name):\n",
    "    disp_conf_matrix.savefig(OUTPUT_PATH_TRAIN_EVAL + \"/conf_matrix_\"+str(classifier_name)+\".jpg\")\n",
    "    print(\"file successfully stored in: output/ml_models/02_training_set_evaluation\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# saving the confusion matrix\n",
    "store_conf_matrix(disp_conf_matrix, \"LogisticRegression\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inspecting the errors\n",
    "output_dict = {}\n",
    "output_array = np.c_[pic_ids_val, labels_val, pred_val] ## adjust name of pic_ids and labels depending on train or val\n",
    "\n",
    "# Create error array with specific error\n",
    "err_type_arr = np.array([])\n",
    "for i in range(len(output_array)):\n",
    "    if output_array[i,1] != output_array[i,2]:\n",
    "        err_type_arr = np.append(err_type_arr, \"error\")\n",
    "    else:\n",
    "        err_type_arr = np.append(err_type_arr, \"No error\")\n",
    "\n",
    "error_table_pd = pd.DataFrame(output_array)\n",
    "error_table_pd.rename(columns = {0:'Picture ID', 1:\"Label\", 2:\"Predicted\"}, inplace = True)\n",
    "error_table_pd[\"Error Check\"] = err_type_arr\n",
    "\n",
    "# print filtered error table\n",
    "print(error_table_pd.loc[error_table_pd[\"Error Check\"].isin([\"error\"])].sort_values(by=[\"Label\", \"Picture ID\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def function for saving the filtered error table\n",
    "\n",
    "def store_error_table(image_df, classifier_name):\n",
    "    savez_compressed(OUTPUT_PATH_TRAIN_EVAL + \"/error_table_\"+str(classifier_name)+\".npz\",image_df)\n",
    "    print(\"file successfully stored in: output/ml_models/02_training_set_evaluation\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transform pd frame into dictionary for saving\n",
    "output_dict[\"error_table\"] = error_table_pd\n",
    "\n",
    "# saving the error table\n",
    "store_error_table(output_dict, \"LogisticRegression\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementing RandomForest Classifier as advanced model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_trees = 500\n",
    "\n",
    "# this works in 5 minutes locally after PCA\n",
    "\n",
    "classifier_RF = RandomForestClassifier(max_depth = 10, n_estimators = num_trees, n_jobs=-1,random_state=42)\n",
    "train_clasf(classifier_RF, img_data_train, labels_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating RandomForest Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# store predictions of classifier ## Victor: ich war mir nicht ganz sicher ob das so richtig ist\n",
    "pred_train = classifier_RF.predict(img_data_train)\n",
    "pred_val = classifier_RF.predict(img_data_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluate classifier and store metrics\n",
    "evaluation_scores = {\"Precision Score Train\": precision_score(labels_train, pred_train, average = \"macro\").round(3),\n",
    "                     \"Precision Score Validation\": precision_score(labels_val, pred_val, average = \"macro\").round(3),\n",
    "                     \"Recall Score Train\": recall_score(labels_train, pred_train, average = \"macro\").round(3),\n",
    "                     \"Recall Score Validation\": recall_score(labels_val, pred_val, average = \"macro\").round(3),\n",
    "                     \"F1 Score Train\": f1_score(labels_train, pred_train, average=\"macro\").round(3),\n",
    "                     \"F1 Score Validation\": f1_score(labels_val, pred_val, average=\"macro\").round(3)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "store_eval_score(evaluation_scores, \"RandomForest\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inspecting the errors\n",
    "output_dict = {}\n",
    "output_array = np.c_[pic_ids_val, labels_val, pred_val] ## adjust name of pic_ids and labels depending on train or val\n",
    "\n",
    "# Create error array with specific error\n",
    "err_type_arr = np.array([])\n",
    "for i in range(len(output_array)):\n",
    "    if output_array[i,1] != output_array[i,2]:\n",
    "        err_type_arr = np.append(err_type_arr, \"error\")\n",
    "    else:\n",
    "        err_type_arr = np.append(err_type_arr, \"No error\")\n",
    "\n",
    "error_table_pd = pd.DataFrame(output_array)\n",
    "error_table_pd.rename(columns = {0:'Picture ID', 1:\"Label\", 2:\"Predicted\"}, inplace = True)\n",
    "error_table_pd[\"Error Check\"] = err_type_arr\n",
    "\n",
    "# print filtered error table\n",
    "print(error_table_pd.loc[error_table_pd[\"Error Check\"].isin([\"error\"])].sort_values(by=[\"Label\", \"Picture ID\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transform pd frame into dictionary for saving\n",
    "output_dict[\"error_table\"] = error_table_pd\n",
    "\n",
    "# saving the error table\n",
    "store_error_table(output_dict, \"RandomForest\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Multiclass Confusion Matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(labels_val, pred_val)\n",
    "cm = confusion_matrix(np.arange(26), np.arange(26))\n",
    "cmp = ConfusionMatrixDisplay(cm, display_labels=np.arange(26))\n",
    "fig, ax = plt.subplots(figsize=(10,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cmp.plot(ax=ax)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay.from_predictions(labels_val, pred_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "disp = disp.plt()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# saving the confusion matrix\n",
    "store_conf_matrix(fig, \"RandomForest\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparamter Tuning RandomForest Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Selecting Parameters of Random Forest Classifier for Grid Search\n",
    "\n",
    "params_grid_RF = [\n",
    "    {\"n_estimators\": [100, 500, 1000, 2000],\n",
    "     \"criterion\":[\"gini\",\"entropy\"],\n",
    "     \"max_features\": [\"auto\", \"log2\", \"sqrt\"],\n",
    "     \"bootstrap\": [True, False],\n",
    "     \"max_depth\": [90, 100, 110],\n",
    "     \"min_samples_leaf\": [1, 2, 4],\n",
    "     \"min_samples_split\": [2, 5, 10]}\n",
    "]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classifier_RF_hyper = RandomForestClassifier()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform Randomized Search\n",
    "\n",
    "scoring = [\"f1_macro\", \"accuracy\", \"precision_macro\", \"recall_macro\"]\n",
    "\n",
    "\n",
    "randomized_search = RandomizedSearchCV(estimator = classifier_RF_hyper,\n",
    "                           param_distributions = params_grid_RF,\n",
    "                           n_iter=10,\n",
    "                           cv = 3,\n",
    "                           scoring= scoring,\n",
    "                           refit = \"precision_macro\",\n",
    "                           n_jobs = -1,\n",
    "                           verbose = 3,\n",
    "                           return_train_score = True)\n",
    "\n",
    "randomized_search.fit(img_data_train, labels_train)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform Grid-Search\n",
    "#Takes too loooong\n",
    "\n",
    "scoring = [\"f1_macro\", \"precision_macro\", \"recall_macro\"]\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator = classifier_RF_hyper,\n",
    "                           param_grid = params_grid_RF,\n",
    "                           cv = 3,\n",
    "                           scoring= scoring,\n",
    "                           refit = \"precision_macro\",\n",
    "                           n_jobs = -1,\n",
    "                           verbose = 3,\n",
    "                           return_train_score = True)\n",
    "\n",
    "grid_search.fit(img_data_train, labels_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(randomized_search.best_params_)\n",
    "best_hyppar_dict = {}\n",
    "best_hyppar_dict[\"Randomized_Search_Best_Params\"] = randomized_search.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def store_best_hyperpar(image_df, classifier_name):\n",
    "    savez_compressed(OUTPUT_PATH_HYPPAR_TUN + \"/best_hyperpar_\"+str(classifier_name)+\".npz\",image_df)\n",
    "    print(\"file successfully stored in: output/ml_models/04_hyperparamter_tuning\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_params_dict = best_hyppar_dict\n",
    "store_best_hyperpar(best_params_dict, \"RandomForestTuned\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate tuned RF classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## define the tuned classifier with the best hyperparamters\n",
    "## these here are still random ones\n",
    "\n",
    "num_trees = 10 # change to actual value\n",
    "\n",
    "classifier_RF_tuned = RandomForestClassifier(class_weight='balanced',\n",
    "                                      criterion='gini',\n",
    "                                      max_depth=55,\n",
    "                                      max_features='log2',\n",
    "                                      min_samples_leaf=0.005,\n",
    "                                      min_samples_split=0.005,\n",
    "                                      n_estimators=num_trees)\n",
    "\n",
    "classifier_RF_tuned.fit(img_data_train,labels_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# store predictions of classifier\n",
    "pred_train = classifier_RF_tuned.predict(img_data_train)\n",
    "pred_val = classifier_RF_tuned.predict(img_data_val)\n",
    "\n",
    "# evaluate classifier and store metrics\n",
    "evaluation_scores = {\"Precision Score Train\": precision_score(labels_train, pred_train, average = \"macro\").round(3),\n",
    "                     \"Precision Score Validation\": precision_score(labels_val, pred_val, average = \"macro\").round(3),\n",
    "                     \"Recall Score Train\": recall_score(labels_train, pred_train, average = \"macro\").round(3),\n",
    "                     \"Recall Score Validation\": recall_score(labels_val, pred_val, average = \"macro\").round(3),\n",
    "                     \"F1 Score Train\": f1_score(labels_train, pred_train, average=\"macro\").round(3),\n",
    "                     \"F1 Score Validation\": f1_score(labels_val, pred_val, average=\"macro\").round(3)}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "store_eval_score(evaluation_scores, \"RandomForest_tuned\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Multiclass Confusion Matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking Feature importance of RF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This needs to be done on the full image data (pre-PCA)\n",
    "## define the tuned classifier with the best hyperparamters\n",
    "## these here are still random ones\n",
    "\n",
    "num_trees = 10 # change to actual value\n",
    "\n",
    "classifier_RF_tuned_no_PCA = RandomForestClassifier(class_weight='balanced',\n",
    "                                      criterion='gini',\n",
    "                                      max_depth=55,\n",
    "                                      max_features='log2',\n",
    "                                      min_samples_leaf=0.005,\n",
    "                                      min_samples_split=0.005,\n",
    "                                      n_estimators=num_trees)\n",
    "\n",
    "classifier_RF_tuned.fit(img_data_train_no_PCA,labels_train_no_PCA)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining the function to plot the digits of feature importance\n",
    "# Adapted from Aurelien Geron:\n",
    "\n",
    "pix_res = 48\n",
    "\n",
    "def plot_digit(data):\n",
    "    image = data.reshape(pix_res, pix_res)\n",
    "    plt.imshow(image, cmap = mpl.cm.hot,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adding feature importances from 3 RGB values to one pixel\n",
    "feature_imp_sum = np.empty([(int(len(classifier_RF_tuned_no_PCA.feature_importances_)/3)),])\n",
    "\n",
    "for itr in range(int(len(classifier_RF_tuned_no_PCA.feature_importances_)/3)):\n",
    "    r = int(itr*3)\n",
    "    g = int(r+1)\n",
    "    b = int(g+1)\n",
    "    feature_imp_sum[itr] = classifier_RF_tuned_no_PCA.feature_importances_[r] + classifier_RF_tuned_no_PCA.feature_importances_[g] + classifier_RF_tuned_no_PCA.feature_importances_[b]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting feature importance sum for every pixel to a plot and save it\n",
    "plot_digit(feature_imp_sum)\n",
    "\n",
    "cbar = plt.colorbar(ticks=[feature_imp_sum.min(), feature_imp_sum.max()])\n",
    "cbar.ax.set_yticklabels(['Not important', 'Very important'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_fig(\"RandomForest_feature_importance_plot_full_data\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementing XGBoost as second advanced model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "classifier_XGB = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42, eval_metric=\"auc\")\n",
    "classifier_XGB.fit(img_data_train, labels_train, early_stopping_rounds=5, eval_set=[(img_data_val, labels_val)])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating XGBoost Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# store predictions of classifier\n",
    "pred_train = classifier_XGB.predict(img_data_train)\n",
    "pred_val = classifier_XGB.predict(img_data_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluate classifier and store metrics\n",
    "evaluation_scores = {\"Precision Score Train\": precision_score(labels_train, pred_train, average = \"macro\").round(3),\n",
    "                     \"Precision Score Validation\": precision_score(labels_val, pred_val, average = \"macro\").round(3),\n",
    "                     \"Recall Score Train\": recall_score(labels_train, pred_train, average = \"macro\").round(3),\n",
    "                     \"Recall Score Validation\": recall_score(labels_val, pred_val, average = \"macro\").round(3),\n",
    "                     \"F1 Score Train\": f1_score(labels_train, pred_train, average=\"macro\").round(3),\n",
    "                     \"F1 Score Validation\": f1_score(labels_val, pred_val, average=\"macro\").round(3)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "store_eval_score(evaluation_scores, \"XGBoost\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inspecting the errors\n",
    "output_dict = {}\n",
    "output_array = np.c_[pic_ids_val, labels_val, pred_val] ## adjust name of pic_ids and labels depending on train or val\n",
    "\n",
    "# Create error array with specific error\n",
    "err_type_arr = np.array([])\n",
    "for i in range(len(output_array)):\n",
    "    if output_array[i,1] != output_array[i,2]:\n",
    "        err_type_arr = np.append(err_type_arr, \"error\")\n",
    "    else:\n",
    "        err_type_arr = np.append(err_type_arr, \"No error\")\n",
    "\n",
    "error_table_pd = pd.DataFrame(output_array)\n",
    "error_table_pd.rename(columns = {0:'Picture ID', 1:\"Label\", 2:\"Predicted\"}, inplace = True)\n",
    "error_table_pd[\"Error Check\"] = err_type_arr\n",
    "\n",
    "# print filtered error table\n",
    "print(error_table_pd.loc[error_table_pd[\"Error Check\"].isin([\"error\"])].sort_values(by=[\"Label\", \"Picture ID\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transform pd frame into dictionary for saving\n",
    "output_dict[\"error_table\"] = error_table_pd\n",
    "\n",
    "# saving the error table\n",
    "store_error_table(output_dict, \"XGBoost\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparamter Tuning XGBoost Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import uniform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Selecting Parameters of XGBoost Classifier for Grid Search\n",
    "\n",
    "params_grid_XGB = [{\n",
    "    \"colsample_bytree\": uniform(0.7, 0.3),\n",
    "    \"gamma\": uniform(0, 0.5),\n",
    "    \"learning_rate\": uniform(0.03, 0.3), # default 0.1\n",
    "    \"max_depth\": [15, 20 25],\n",
    "    \"n_estimators\": [400, 700, 1000],\n",
    "    \"subsample\": uniform(0.6, 0.4)}\n",
    "]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classifier_XGB_hyper = xgb.XGBClassifier()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform Grid-Search\n",
    "\n",
    "scoring = [\"f1_macro\", \"precision_macro\", \"recall_macro\"]\n",
    "\n",
    "grid_search = GridSearchCV(estimator = classifier_XGB_hyper,\n",
    "                           param_grid = params_grid_XGB,\n",
    "                           cv = 3,\n",
    "                           scoring= scoring,\n",
    "                           refit = \"precision\",\n",
    "                           n_jobs = -1,\n",
    "                           verbose = 3,\n",
    "                           return_train_score = True)\n",
    "\n",
    "grid_search.fit(img_data_train, labels_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)\n",
    "best_hyppar_dict = {}\n",
    "best_hyppar_dict[\"Grid_Search_Best_Params\"] = grid_search.best_params_\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_params_dict = best_hyppar_dict\n",
    "store_best_hyperpar(best_params_dict, \"XGBoostTuned\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate tuned XGBoost classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## define the tuned classifier with the best hyperparamters\n",
    "## these here are still random ones\n",
    "\n",
    "classifier_XGB_tuned = xgb.XGBClassifier(colsample_bytree=0.7, gamma=0.5, learning_rate=0.3,max_depth=25, subsample=0.5,n_estimators=700)\n",
    "\n",
    "classifier_XGB_tuned.fit(img_data_train,labels_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# store predictions of classifier\n",
    "pred_train = classifier_XGB_tuned.predict(img_data_train)\n",
    "pred_val = classifier_XGB_tuned.predict(img_data_val)\n",
    "\n",
    "# evaluate classifier and store metrics\n",
    "evaluation_scores = {\"Precision Score Train\": precision_score(labels_train, pred_train, average = \"macro\").round(3),\n",
    "                     \"Precision Score Validation\": precision_score(labels_val, pred_val, average = \"macro\").round(3),\n",
    "                     \"Recall Score Train\": recall_score(labels_train, pred_train, average = \"macro\").round(3),\n",
    "                     \"Recall Score Validation\": recall_score(labels_val, pred_val, average = \"macro\").round(3),\n",
    "                     \"F1 Score Train\": f1_score(labels_train, pred_train, average=\"macro\").round(3),\n",
    "                     \"F1 Score Validation\": f1_score(labels_val, pred_val, average=\"macro\").round(3)}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "store_eval_score(evaluation_scores, \"XGBoost_tuned\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Multiclass Confusion Matrix\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Speed improvements through dimensionality reduction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking Feature importance of RF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining the function to plot the digits of feature importance\n",
    "# Adapted from Aurelien Geron:\n",
    "\n",
    "pix_res = 48\n",
    "\n",
    "def plot_digit(data):\n",
    "    image = data.reshape(pix_res, pix_res)\n",
    "    plt.imshow(image, cmap = mpl.cm.hot,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adding feature importances from 3 RGB values to one pixel\n",
    "feature_imp_sum = np.empty([(int(len(classifier_RF_tuned.feature_importances_)/3)),])\n",
    "\n",
    "for itr in range(int(len(classifier_RF_tuned.feature_importances_)/3)):\n",
    "    r = int(itr*3)\n",
    "    g = int(r+1)\n",
    "    b = int(g+1)\n",
    "    feature_imp_sum[itr] = classifier_RF_tuned.feature_importances_[r] + classifier_RF_tuned.feature_importances_[g] + classifier_RF_tuned.feature_importances_[b]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting feature importance sum for every pixel to a plot and save it\n",
    "plot_digit(feature_imp_sum)\n",
    "\n",
    "cbar = plt.colorbar(ticks=[feature_imp_sum.min(), feature_imp_sum.max()])\n",
    "cbar.ax.set_yticklabels(['Not important', 'Very important'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_fig(\"RandomForest_feature_importance_plot_full_data\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Performing Principal Component Analysis on the training data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_data_full.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# defining to keep 99% of the variance of the data\n",
    "pca = PCA(.99)\n",
    "img_data_full_red = pca.fit_transform(img_data_full)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# checking how many features are left\n",
    "img_data_full_red.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split into training and test set - 10.000 test set / 40.000 full training set\n",
    "# stratify = labels splits it proportionally to classes in the dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "img_data_train_full, img_data_test, labels_train_full, labels_test, pic_ids_train_full, pic_ids_test = train_test_split(img_data_full_red , labels_full, pic_ids_full, stratify=labels_full, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split into training and validation set - 30.000 training set / 10.000 validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "img_data_train, img_data_val, labels_train, labels_val, pic_ids_train, pic_ids_val = train_test_split(img_data_train_full , labels_train_full, pic_ids_train_full, train_size=30000, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate speed tuned RF classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Victor: muss man dann hier nochmal das ganze training (direkt mit den gefundenen hyperparametern) und\n",
    "# evaluation nochmal machen? Schon oder?\n",
    "# und dann besteht die Gefahr, dass die Hyperparameter nicht mehr ideal sind und man die dann nochmal\n",
    "# fine tunen muss (so wars bei Thilo) aber falls die predictions nicht zu schlecht sind können wir auch argumentieren\n",
    "# dass wir das aufgrund von run-time and scope limitations of our project nicht mehr machen"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## define the tuned classifier with the best hyperparamters\n",
    "## these here are still random ones\n",
    "\n",
    "num_trees = 10 # change to actual value\n",
    "\n",
    "classifier_RF_tuned_PCA = RandomForestClassifier(class_weight='balanced',\n",
    "                                      criterion='gini',\n",
    "                                      max_depth=55,\n",
    "                                      max_features='log2',\n",
    "                                      min_samples_leaf=0.005,\n",
    "                                      min_samples_split=0.005,\n",
    "                                      n_estimators=num_trees)\n",
    "\n",
    "classifier_RF_tuned_PCA.fit(img_data_train,labels_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# store predictions of classifier\n",
    "pred_train = classifier_RF_tuned_PCA.predict(img_data_train)\n",
    "pred_val = classifier_RF_tuned_PCA.predict(img_data_val)\n",
    "\n",
    "# evaluate classifier and store metrics\n",
    "evaluation_scores = {\"Precision Score Train\": precision_score(labels_train, pred_train, average = \"macro\").round(3),\n",
    "                     \"Precision Score Validation\": precision_score(labels_val, pred_val, average = \"macro\").round(3),\n",
    "                     \"Recall Score Train\": recall_score(labels_train, pred_train, average = \"macro\").round(3),\n",
    "                     \"Recall Score Validation\": recall_score(labels_val, pred_val, average = \"macro\").round(3),\n",
    "                     \"F1 Score Train\": f1_score(labels_train, pred_train, average=\"macro\").round(3),\n",
    "                     \"F1 Score Validation\": f1_score(labels_val, pred_val, average=\"macro\").round(3)}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "store_eval_score(evaluation_scores, \"RandomForest_tuned_PCA\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate speed tuned XGB classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## define the tuned classifier with the best hyperparamters\n",
    "## these here are still random ones\n",
    "\n",
    "classifier_XGB_tuned_PCA = xgb.XGBClassifier(colsample_bytree=0.7, gamma=0.5, learning_rate=0.3,max_depth=25, subsample=0.5,n_estimators=700)\n",
    "\n",
    "classifier_XGB_tuned_PCA.fit(img_data_train,labels_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# store predictions of classifier\n",
    "pred_train = classifier_XGB_tuned_PCA.predict(img_data_train)\n",
    "pred_val = classifier_XGB_tuned_PCA.predict(img_data_val)\n",
    "\n",
    "# evaluate classifier and store metrics\n",
    "evaluation_scores = {\"Precision Score Train\": precision_score(labels_train, pred_train, average = \"macro\").round(3),\n",
    "                     \"Precision Score Validation\": precision_score(labels_val, pred_val, average = \"macro\").round(3),\n",
    "                     \"Recall Score Train\": recall_score(labels_train, pred_train, average = \"macro\").round(3),\n",
    "                     \"Recall Score Validation\": recall_score(labels_val, pred_val, average = \"macro\").round(3),\n",
    "                     \"F1 Score Train\": f1_score(labels_train, pred_train, average=\"macro\").round(3),\n",
    "                     \"F1 Score Validation\": f1_score(labels_val, pred_val, average=\"macro\").round(3)}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "store_eval_score(evaluation_scores, \"XGBoost_tuned_PCA\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final Evaluation on Test Set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## test baseline RF on test set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# store predictions of classifier\n",
    "\n",
    "pred_test = classifier_LR.predict(img_data_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluate classifier and store metrics\n",
    "\n",
    "evaluation_scores = {\"Precision Score Test\": precision_score(labels_test, pred_test, average = \"macro\").round(3),\n",
    "    \"Recall Score Test\": recall_score(labels_test, pred_test, average = \"macro\").round(3),\n",
    "    \"F1 Score Test\": f1_score(labels_test, pred_test, average=\"macro\").round(3)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save evaluation scores\n",
    "\n",
    "def store_eval_score_test(image_df, classifier_name):\n",
    "    savez_compressed(OUTPUT_PATH_TEST_EVAL + \"/evaluation_scores_\"+str(classifier_name)+\".npz\",image_df)\n",
    "    print(\"file successfully stored in: output/ml_models/03_test_set_evaluation\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "store_eval_score_test(evaluation_scores,\"Logistic Regression\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## test tuned RF on test set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# store predictions of classifier\n",
    "\n",
    "pred_test = classifier_RF_tuned.predict(img_data_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluate classifier and store metrics\n",
    "\n",
    "evaluation_scores = {\"Precision Score Test\": precision_score(labels_test, pred_test, average = \"macro\").round(3),\n",
    "                     \"Recall Score Test\": recall_score(labels_test, pred_test, average = \"macro\").round(3),\n",
    "                     \"F1 Score Test\": f1_score(labels_test, pred_test, average=\"macro\").round(3)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# store evaluation scores\n",
    "\n",
    "store_eval_score_test(evaluation_scores,\"Tuned Random Forest\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### test tuned XGBoost on test set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# store predictions of classifier\n",
    "\n",
    "pred_test = classifier_XGB_tuned_PCA.predict(img_data_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluate classifier and store metrics\n",
    "evaluation_scores = {\"Precision Score Test\": precision_score(labels_test, pred_test, average = \"macro\").round(3),\n",
    "                     \"Recall Score Test\": recall_score(labels_test, pred_test, average = \"macro\").round(3),\n",
    "                     \"F1 Score Test\": f1_score(labels_test, pred_test, average=\"macro\").round(3)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# store evaluation scores\n",
    "\n",
    "store_eval_score_test(evaluation_scores,\"Tuned XGBoost\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}