{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8mZXfVsLb-D"
   },
   "source": [
    "# [Reference](https://keras.io/examples/vision/mobilevit/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHALHIZ4Lb-E"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this example, we implement the MobileViT architecture (Mehta et al.), which combines the benefits of Transformers (Vaswani et al.) and convolutions. With Transformers, we can capture long-range dependencies that result in global representations. With convolutions, we can capture spatial relationships that model locality.\n",
    "\n",
    "Besides combining the properties of Transformers and convolutions, the authors introduce MobileViT as a general-purpose mobile-friendly backbone for different image recognition tasks. Their findings suggest that, performance-wise, MobileViT is better than other models with the same or higher complexity (MobileNetV3, for example), while being efficient on mobile devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8X7oY7U6M0Z9",
    "outputId": "4b6373a3-d458-4ebd-a9ca-3a2973d736f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.6.0\r\n",
      "  Downloading tensorflow-2.6.0-cp39-cp39-macosx_10_11_x86_64.whl (199.0 MB)\r\n",
      "     |████████████████████████████████| 199.0 MB 13.8 MB/s            \r\n",
      "\u001B[?25hCollecting termcolor~=1.1.0\r\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting tensorflow-estimator~=2.6\r\n",
      "  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\r\n",
      "     |████████████████████████████████| 462 kB 16.3 MB/s            \r\n",
      "\u001B[?25hCollecting keras~=2.6\r\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\r\n",
      "     |████████████████████████████████| 1.4 MB 17.4 MB/s            \r\n",
      "\u001B[?25hCollecting clang~=5.0\r\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting opt-einsum~=3.3.0\r\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\r\n",
      "     |████████████████████████████████| 65 kB 8.9 MB/s             \r\n",
      "\u001B[?25hCollecting flatbuffers~=1.12.0\r\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\r\n",
      "Collecting six~=1.15.0\r\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\r\n",
      "Collecting grpcio<2.0,>=1.37.0\r\n",
      "  Downloading grpcio-1.44.0-cp39-cp39-macosx_10_10_x86_64.whl (4.3 MB)\r\n",
      "     |████████████████████████████████| 4.3 MB 11.9 MB/s            \r\n",
      "\u001B[?25hCollecting tensorboard~=2.6\r\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\r\n",
      "     |████████████████████████████████| 5.8 MB 12.4 MB/s            \r\n",
      "\u001B[?25hCollecting gast==0.4.0\r\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\r\n",
      "Collecting absl-py~=0.10\r\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\r\n",
      "     |████████████████████████████████| 132 kB 14.7 MB/s            \r\n",
      "\u001B[?25hCollecting keras-preprocessing~=1.1.2\r\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\r\n",
      "     |████████████████████████████████| 42 kB 6.0 MB/s             \r\n",
      "\u001B[?25hRequirement already satisfied: wheel~=0.35 in ./venv/lib/python3.9/site-packages (from tensorflow==2.6.0) (0.37.1)\r\n",
      "Collecting astunparse~=1.6.3\r\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\n",
      "Collecting h5py~=3.1.0\r\n",
      "  Downloading h5py-3.1.0-cp39-cp39-macosx_10_9_x86_64.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 15.3 MB/s            \r\n",
      "\u001B[?25hCollecting google-pasta~=0.2\r\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\n",
      "     |████████████████████████████████| 57 kB 12.0 MB/s            \r\n",
      "\u001B[?25hCollecting typing-extensions~=3.7.4\r\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\r\n",
      "Collecting wrapt~=1.12.1\r\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting numpy~=1.19.2\r\n",
      "  Downloading numpy-1.19.5-cp39-cp39-macosx_10_9_x86_64.whl (15.6 MB)\r\n",
      "     |████████████████████████████████| 15.6 MB 17.0 MB/s            \r\n",
      "\u001B[?25hCollecting protobuf>=3.9.2\r\n",
      "  Downloading protobuf-3.20.0-cp39-cp39-macosx_10_9_x86_64.whl (962 kB)\r\n",
      "     |████████████████████████████████| 962 kB 16.4 MB/s            \r\n",
      "\u001B[?25hCollecting tensorboard-plugin-wit>=1.6.0\r\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\r\n",
      "     |████████████████████████████████| 781 kB 14.7 MB/s            \r\n",
      "\u001B[?25hRequirement already satisfied: setuptools>=41.0.0 in ./venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.6.0) (60.9.3)\r\n",
      "Collecting markdown>=2.6.8\r\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\r\n",
      "     |████████████████████████████████| 97 kB 12.6 MB/s            \r\n",
      "\u001B[?25hRequirement already satisfied: requests<3,>=2.21.0 in /Users/reedgarvin/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.6->tensorflow==2.6.0) (2.27.1)\r\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\r\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\r\n",
      "     |████████████████████████████████| 3.5 MB 15.7 MB/s            \r\n",
      "\u001B[?25hRequirement already satisfied: werkzeug>=0.11.15 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.6.0) (2.0.2)\r\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\r\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\r\n",
      "Collecting google-auth<3,>=1.6.3\r\n",
      "  Downloading google_auth-2.6.3-py2.py3-none-any.whl (156 kB)\r\n",
      "     |████████████████████████████████| 156 kB 16.7 MB/s            \r\n",
      "\u001B[?25hCollecting rsa<5,>=3.1.4\r\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\r\n",
      "Collecting cachetools<6.0,>=2.0.0\r\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\r\n",
      "Collecting pyasn1-modules>=0.2.1\r\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\n",
      "     |████████████████████████████████| 155 kB 17.4 MB/s            \r\n",
      "\u001B[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /Users/reedgarvin/Library/Python/3.9/lib/python/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6.0) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.6.0) (4.8.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (2021.10.8)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (2.0.9)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (1.26.7)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.6.0) (3.6.0)\r\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\r\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\r\n",
      "     |████████████████████████████████| 77 kB 13.3 MB/s            \r\n",
      "\u001B[?25hRequirement already satisfied: oauthlib>=3.0.0 in /Users/reedgarvin/Library/Python/3.9/lib/python/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6.0) (3.2.0)\r\n",
      "Building wheels for collected packages: clang, termcolor, wrapt\r\n",
      "  Building wheel for clang (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30693 sha256=2115cc508381e0e8ba8e1cfbb444c37f5f2f6c65aa4e682d5eed499bfeed24b6\r\n",
      "  Stored in directory: /Users/reedgarvin/Library/Caches/pip/wheels/3a/ce/7a/27094f689461801c934296d07078773603663dfcaca63bb064\r\n",
      "  Building wheel for termcolor (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=d419798919a9b7e92312241c7e81b2514e3a63d8133c2c977bb93b1bafe19e5c\r\n",
      "  Stored in directory: /Users/reedgarvin/Library/Caches/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\r\n",
      "  Building wheel for wrapt (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp39-cp39-macosx_10_9_x86_64.whl size=32790 sha256=d7f1d70ecf1b8bf196f8144e0eadc54c3f26447c17371d4feb2490bc2636b150\r\n",
      "  Stored in directory: /Users/reedgarvin/Library/Caches/pip/wheels/98/23/68/efe259aaca055e93b08e74fbe512819c69a2155c11ba3c0f10\r\n",
      "Successfully built clang termcolor wrapt\r\n",
      "Installing collected packages: pyasn1, six, rsa, pyasn1-modules, cachetools, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, clang, astunparse, tensorflow\r\n",
      "  Attempting uninstall: six\r\n",
      "    Found existing installation: six 1.16.0\r\n",
      "    Not uninstalling six at /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages, outside environment /Users/reedgarvin/Documents/GitHub/happy-whale/venv\r\n",
      "    Can't uninstall 'six'. No files were found to uninstall.\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.21.4\r\n",
      "    Not uninstalling numpy at /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages, outside environment /Users/reedgarvin/Documents/GitHub/happy-whale/venv\r\n",
      "    Can't uninstall 'numpy'. No files were found to uninstall.\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing-extensions 4.0.1\r\n",
      "    Not uninstalling typing-extensions at /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages, outside environment /Users/reedgarvin/Documents/GitHub/happy-whale/venv\r\n",
      "    Can't uninstall 'typing-extensions'. No files were found to uninstall.\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pyldavis 3.3.1 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\r\n",
      "black 22.1.0 requires typing-extensions>=3.10.0.0; python_version < \"3.10\", but you have typing-extensions 3.7.4.3 which is incompatible.\u001B[0m\r\n",
      "Successfully installed absl-py-0.15.0 astunparse-1.6.3 cachetools-5.0.0 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-auth-2.6.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 h5py-3.1.0 keras-2.8.0 keras-preprocessing-1.1.2 markdown-3.3.6 numpy-1.19.5 opt-einsum-3.3.0 protobuf-3.20.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 rsa-4.8 six-1.15.0 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.6.0 tensorflow-estimator-2.8.0 termcolor-1.1.0 typing-extensions-3.7.4.3 wrapt-1.12.1\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\r\n",
      "You should consider upgrading via the '/Users/reedgarvin/Documents/GitHub/happy-whale/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Collecting tensorflow-addons\r\n",
      "  Downloading tensorflow_addons-0.16.1-cp39-cp39-macosx_10_15_x86_64.whl (588 kB)\r\n",
      "     |████████████████████████████████| 588 kB 3.0 MB/s            \r\n",
      "\u001B[?25hCollecting typeguard>=2.7\r\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\r\n",
      "Installing collected packages: typeguard, tensorflow-addons\r\n",
      "Successfully installed tensorflow-addons-0.16.1 typeguard-2.13.3\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\r\n",
      "You should consider upgrading via the '/Users/reedgarvin/Documents/GitHub/happy-whale/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Collecting tensorflow-hub\r\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\r\n",
      "     |████████████████████████████████| 108 kB 3.1 MB/s            \r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.12.0 in ./venv/lib/python3.9/site-packages (from tensorflow-hub) (1.19.5)\r\n",
      "Requirement already satisfied: protobuf>=3.8.0 in ./venv/lib/python3.9/site-packages (from tensorflow-hub) (3.20.0)\r\n",
      "Installing collected packages: tensorflow-hub\r\n",
      "Successfully installed tensorflow-hub-0.12.0\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\r\n",
      "You should consider upgrading via the '/Users/reedgarvin/Documents/GitHub/happy-whale/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\r\n",
      "\u001B[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu111 (from versions: 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0)\u001B[0m\r\n",
      "\u001B[31mERROR: No matching distribution found for torch==1.9.0+cu111\u001B[0m\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\r\n",
      "You should consider upgrading via the '/Users/reedgarvin/Documents/GitHub/happy-whale/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "#allow to work on hertie server\n",
    "#os.environ['TORCH_HOME']= '/workspace/cache'\n",
    "!pip3 install -U tensorflow==2.6.0\n",
    "!pip3 install tensorflow-addons\n",
    "!pip3 install tensorflow-hub\n",
    "\n",
    "#!pip3 install --upgrade typing-extensions\n",
    "#!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:09.410745Z",
     "iopub.status.busy": "2022-03-17T09:07:09.410483Z",
     "iopub.status.idle": "2022-03-17T09:07:13.503386Z",
     "shell.execute_reply": "2022-03-17T09:07:13.502614Z",
     "shell.execute_reply.started": "2022-03-17T09:07:09.410678Z"
    },
    "id": "HVhAp0s6Lb-F",
    "outputId": "c1b6c7fe-8a97-49bb-d1b7-9df946bd256b"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m \n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m \n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import os, cv2, gc\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras, Module \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow_hub as hub\n",
    "from keras.applications import imagenet_utils\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow_addons as tfa\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "try:\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    pass \n",
    "\n",
    "# enable mixed_precision and jit compiler \n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "tf.config.optimizer.set_jit(True)\n",
    "#allow gpu to work \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.505817Z",
     "iopub.status.busy": "2022-03-17T09:07:13.505527Z",
     "iopub.status.idle": "2022-03-17T09:07:13.512389Z",
     "shell.execute_reply": "2022-03-17T09:07:13.511717Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.505782Z"
    },
    "id": "K1cm2VNBLb-G"
   },
   "outputs": [],
   "source": [
    "INP_SIZE      = (512, 512) # Input size of the Image Resizer Module (IRM)\n",
    "TARGET_SIZE   = (224, 224) # Output size of IRM and Input size of the Vision Transformer \n",
    "INTERPOLATION = \"bilinear\"\n",
    "N_CLASSES = 15587\n",
    "#config_head = 'arcface'\n",
    "\n",
    "\n",
    "NUM_FOLDS  = 5\n",
    "BATCH_SIZE = 24\n",
    "SEED       = 42\n",
    "\n",
    "DATA_DIR  = '../happy-whale/input/'\n",
    "TRAIN_DIR = DATA_DIR + 'cropped_train_images/cropped_train_images/'\n",
    "TEST_DIR  = DATA_DIR + 'cropped_test_images/cropped_test_images/'\n",
    "\n",
    "# SetAutoTune\n",
    "AUTOTUNE = tf.data.AUTOTUNE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../happy-whale/input/cropped_train_images/cropped_train_images/'\n",
    "\n",
    "for file in os.listdir(path): \n",
    "    if not file.endswith(\".jpg\"): \n",
    "        img = Image.open(\"{path}/{file}\")\n",
    "        file_name, file_ext = os.path.splitext(file)\n",
    "        img.save('/jpg/{}.jpg'.format(file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjpFqD3eLb-G"
   },
   "source": [
    "# Prepare tf.data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.514318Z",
     "iopub.status.busy": "2022-03-17T09:07:13.513738Z",
     "iopub.status.idle": "2022-03-17T09:07:13.528971Z",
     "shell.execute_reply": "2022-03-17T09:07:13.528200Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.514281Z"
    },
    "id": "tU8L6ff_Lb-G"
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/ipythonx/tf-keras-learning-to-resize-image-for-vit-model/notebook\n",
    "def build_augmenter(is_labelled):\n",
    "    def augment(img):\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_flip_up_down(img)\n",
    "        img = tf.image.random_saturation(img, 0.65, 1.05)\n",
    "        img = tf.image.random_brightness(img, 0.05)\n",
    "        img = tf.image.random_contrast(img, 0.75, 1.05)\n",
    "        img = tf.image.random_hue(img, 0.05)\n",
    "        return img\n",
    "    \n",
    "    def augment_with_labels(img, label):\n",
    "        return augment(img), label\n",
    "    return augment_with_labels if is_labelled else augment\n",
    "\n",
    "def build_decoder(is_labelled, size):\n",
    "    def decode(path):\n",
    "        file_bytes = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(file_bytes, channels = 3)\n",
    "        img = tf.image.resize(img, (size[0], size[1]))\n",
    "        return tf.cast(tf.divide(img, 255.),tf.float32)\n",
    "    \n",
    "    def decode_with_labels(path, label):\n",
    "        label = tf.cast(label, tf.int32)\n",
    "        return decode(path),label\n",
    "    \n",
    "    return decode_with_labels if is_labelled else decode\n",
    "\n",
    "def create_dataset(df, \n",
    "                   batch_size  = 32, \n",
    "                   is_labelled = False, \n",
    "                   augment     = False, \n",
    "                   repeat      = False, \n",
    "                   shuffle     = False,\n",
    "                   size        = INP_SIZE):\n",
    "    decode_fn    = build_decoder(is_labelled, size)\n",
    "    augmenter_fn = build_augmenter(is_labelled)\n",
    "    \n",
    "    # Create Dataset\n",
    "    if is_labelled:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values, df['target_value'].values))\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values))\n",
    "        \n",
    "    dataset = dataset.map(decode_fn, num_parallel_calls = AUTOTUNE)\n",
    "    dataset = dataset.map(augmenter_fn, num_parallel_calls = AUTOTUNE) if augment else dataset\n",
    "    dataset = dataset.repeat() if repeat else dataset\n",
    "    dataset = dataset.shuffle(1024, reshuffle_each_iteration = True) if shuffle else dataset\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.532873Z",
     "iopub.status.busy": "2022-03-17T09:07:13.532462Z",
     "iopub.status.idle": "2022-03-17T09:07:13.540266Z",
     "shell.execute_reply": "2022-03-17T09:07:13.539401Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.532844Z"
    },
    "id": "uYlfV2w1Lb-H"
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.543268Z",
     "iopub.status.busy": "2022-03-17T09:07:13.542644Z",
     "iopub.status.idle": "2022-03-17T09:07:13.713068Z",
     "shell.execute_reply": "2022-03-17T09:07:13.712390Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.543230Z"
    },
    "id": "9IlEkxlMLb-H",
    "outputId": "518e3af4-2e8e-4e0c-a82d-904e5e5dfb9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df: (51033, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>box</th>\n",
       "      <th>Id</th>\n",
       "      <th>target_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>cadddb1636b9</td>\n",
       "      <td>2 116 802 665</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>12348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000562241d384d.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>1a71fbb72250</td>\n",
       "      <td>588 597 3504 1477</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007c33415ce37.jpg</td>\n",
       "      <td>false_killer_whale</td>\n",
       "      <td>60008f293a2b</td>\n",
       "      <td>0 453 3183 1589</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>5842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007d9bca26a99.jpg</td>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>4b00fe572063</td>\n",
       "      <td>1 91 2636 1495</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>4551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00087baf5cef7a.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>8e5253662392</td>\n",
       "      <td>1139 1590 3574 1913</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>8721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image             species individual_id                  box  \\\n",
       "0  00021adfb725ed.jpg  melon_headed_whale  cadddb1636b9        2 116 802 665   \n",
       "1  000562241d384d.jpg      humpback_whale  1a71fbb72250    588 597 3504 1477   \n",
       "2  0007c33415ce37.jpg  false_killer_whale  60008f293a2b      0 453 3183 1589   \n",
       "3  0007d9bca26a99.jpg  bottlenose_dolphin  4b00fe572063       1 91 2636 1495   \n",
       "4  00087baf5cef7a.jpg      humpback_whale  8e5253662392  1139 1590 3574 1913   \n",
       "\n",
       "                                                  Id  target_value  \n",
       "0  ../happy-whale/input/cropped_train_images/crop...         12348  \n",
       "1  ../happy-whale/input/cropped_train_images/crop...          1636  \n",
       "2  ../happy-whale/input/cropped_train_images/crop...          5842  \n",
       "3  ../happy-whale/input/cropped_train_images/crop...          4551  \n",
       "4  ../happy-whale/input/cropped_train_images/crop...          8721  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Train Data\n",
    "train_df = pd.read_csv(f'{DATA_DIR}train.csv')\n",
    "train_df['Id'] = train_df['image'].apply(lambda x: f'{TRAIN_DIR}{x}')\n",
    "\n",
    "# Adjust typos in \"species\" column from Andrada's kernel\n",
    "train_df[\"species\"] = train_df[\"species\"].replace([\"bottlenose_dolpin\", \"kiler_whale\",\n",
    "                                             \"beluga\", \n",
    "                                             \"globis\", \"pilot_whale\"],\n",
    "                                            [\"bottlenose_dolphin\", \"killer_whale\",\n",
    "                                             \"beluga_whale\", \n",
    "                                             \"short_finned_pilot_whale\", \"short_finned_pilot_whale\"])\n",
    "\n",
    "\n",
    "# Set a specific label to be able to perform stratification\n",
    "#train_df['stratify_label'] = train_df['individual_id']\n",
    "\n",
    "train_df['target_value']  = label_encoder.fit_transform(train_df['individual_id'] )\n",
    "\n",
    "# Summary\n",
    "print(f'train_df: {train_df.shape}')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T10:44:00.341686Z",
     "iopub.status.busy": "2022-03-17T10:44:00.341300Z",
     "iopub.status.idle": "2022-03-17T10:44:00.365251Z",
     "shell.execute_reply": "2022-03-17T10:44:00.364457Z",
     "shell.execute_reply.started": "2022-03-17T10:44:00.341645Z"
    },
    "id": "rEDSarr2Lb-I",
    "outputId": "ed9382ce-16f8-4947-f6a5-84cf809791ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>box</th>\n",
       "      <th>Id</th>\n",
       "      <th>target_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>001001f099519f.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>788 1036 2101 1373</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0164d8fd7a7cfe.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>676 640 2517 1144</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>01e775c518a824.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>12 641 2512 1425</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>02278d7a7d207e.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>1279 960 2630 1373</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>02d21d4df0b87f.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>2347 1222 2536 1368</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49109</th>\n",
       "      <td>f653df18636e4b.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>1778 1042 2308 1215</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49437</th>\n",
       "      <td>f8133d5fc02ba1.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>473 1060 2162 1511</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49879</th>\n",
       "      <td>fa2d984b021ec2.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>1485 1080 2194 1252</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50229</th>\n",
       "      <td>fbef59e7c8918c.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>1225 925 2910 1166</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50995</th>\n",
       "      <td>ffc71880c3066b.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>2040 1397 2300 1585</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    image      species individual_id                  box  \\\n",
       "10     001001f099519f.jpg  minke_whale  19fbb960f07d   788 1036 2101 1373   \n",
       "255    0164d8fd7a7cfe.jpg  minke_whale  19fbb960f07d    676 640 2517 1144   \n",
       "345    01e775c518a824.jpg  minke_whale  19fbb960f07d     12 641 2512 1425   \n",
       "393    02278d7a7d207e.jpg  minke_whale  19fbb960f07d   1279 960 2630 1373   \n",
       "528    02d21d4df0b87f.jpg  minke_whale  19fbb960f07d  2347 1222 2536 1368   \n",
       "...                   ...          ...           ...                  ...   \n",
       "49109  f653df18636e4b.jpg  minke_whale  19fbb960f07d  1778 1042 2308 1215   \n",
       "49437  f8133d5fc02ba1.jpg  minke_whale  19fbb960f07d   473 1060 2162 1511   \n",
       "49879  fa2d984b021ec2.jpg  minke_whale  19fbb960f07d  1485 1080 2194 1252   \n",
       "50229  fbef59e7c8918c.jpg  minke_whale  19fbb960f07d   1225 925 2910 1166   \n",
       "50995  ffc71880c3066b.jpg  minke_whale  19fbb960f07d  2040 1397 2300 1585   \n",
       "\n",
       "                                                      Id  target_value  \n",
       "10     ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "255    ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "345    ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "393    ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "528    ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "...                                                  ...           ...  \n",
       "49109  ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "49437  ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "49879  ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "50229  ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "50995  ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "\n",
       "[154 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['individual_id']=='19fbb960f07d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T10:42:12.903388Z",
     "iopub.status.busy": "2022-03-17T10:42:12.903048Z",
     "iopub.status.idle": "2022-03-17T10:42:12.924220Z",
     "shell.execute_reply": "2022-03-17T10:42:12.923342Z",
     "shell.execute_reply.started": "2022-03-17T10:42:12.903349Z"
    },
    "id": "1efHF-bvLb-I",
    "outputId": "7e31bcf8-88ee-4e84-b7c4-96dc2fd57658"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bottlenose_dolphin           10781\n",
       "beluga_whale                  7443\n",
       "humpback_whale                7392\n",
       "blue_whale                    4830\n",
       "false_killer_whale            3326\n",
       "dusky_dolphin                 3139\n",
       "killer_whale                  2455\n",
       "spinner_dolphin               1700\n",
       "melon_headed_whale            1689\n",
       "minke_whale                   1608\n",
       "fin_whale                     1324\n",
       "gray_whale                    1123\n",
       "southern_right_whale           866\n",
       "short_finned_pilot_whale       745\n",
       "spotted_dolphin                490\n",
       "sei_whale                      428\n",
       "common_dolphin                 347\n",
       "cuviers_beaked_whale           341\n",
       "long_finned_pilot_whale        238\n",
       "white_sided_dolphin            229\n",
       "brydes_whale                   154\n",
       "pantropic_spotted_dolphin      145\n",
       "commersons_dolphin              90\n",
       "pygmy_killer_whale              76\n",
       "rough_toothed_dolphin           60\n",
       "frasiers_dolphin                14\n",
       "Name: species, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.species.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.714990Z",
     "iopub.status.busy": "2022-03-17T09:07:13.714497Z",
     "iopub.status.idle": "2022-03-17T09:07:13.743074Z",
     "shell.execute_reply": "2022-03-17T09:07:13.742340Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.714952Z"
    },
    "id": "YVTJeEUnLb-I",
    "outputId": "e6d926b1-cac0-46fd-dfe7-89be159b97b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image                0\n",
       "species              0\n",
       "individual_id        0\n",
       "box              15362\n",
       "Id                   0\n",
       "target_value         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.746177Z",
     "iopub.status.busy": "2022-03-17T09:07:13.745933Z",
     "iopub.status.idle": "2022-03-17T09:07:13.752035Z",
     "shell.execute_reply": "2022-03-17T09:07:13.751288Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.746153Z"
    },
    "id": "TH1VzUwmLb-J"
   },
   "outputs": [],
   "source": [
    "train_ds, val_ds = train_df[:30000], train_df[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.753801Z",
     "iopub.status.busy": "2022-03-17T09:07:13.753336Z",
     "iopub.status.idle": "2022-03-17T09:07:13.830959Z",
     "shell.execute_reply": "2022-03-17T09:07:13.830251Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.753757Z"
    },
    "id": "qWmCYO7oLb-J",
    "outputId": "110e9d6a-9e54-4feb-92bb-5083628ada3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df: (27956, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>predictions</th>\n",
       "      <th>box</th>\n",
       "      <th>Id</th>\n",
       "      <th>individual_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000110707af0ba.jpg</td>\n",
       "      <td>37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...</td>\n",
       "      <td>329 874 2934 1271</td>\n",
       "      <td>../happy-whale/input/cropped_test_images/cropp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0006287ec424cb.jpg</td>\n",
       "      <td>37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...</td>\n",
       "      <td>1039 1636 1833 1875</td>\n",
       "      <td>../happy-whale/input/cropped_test_images/cropp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000809ecb2ccad.jpg</td>\n",
       "      <td>37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...</td>\n",
       "      <td>787 787 1541 948</td>\n",
       "      <td>../happy-whale/input/cropped_test_images/cropp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00098d1376dab2.jpg</td>\n",
       "      <td>37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...</td>\n",
       "      <td>77 228 2029 714</td>\n",
       "      <td>../happy-whale/input/cropped_test_images/cropp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000b8d89c738bd.jpg</td>\n",
       "      <td>37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...</td>\n",
       "      <td>4 20 289 317</td>\n",
       "      <td>../happy-whale/input/cropped_test_images/cropp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image                                        predictions  \\\n",
       "0  000110707af0ba.jpg  37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...   \n",
       "1  0006287ec424cb.jpg  37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...   \n",
       "2  000809ecb2ccad.jpg  37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...   \n",
       "3  00098d1376dab2.jpg  37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...   \n",
       "4  000b8d89c738bd.jpg  37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...   \n",
       "\n",
       "                   box                                                 Id  \\\n",
       "0    329 874 2934 1271  ../happy-whale/input/cropped_test_images/cropp...   \n",
       "1  1039 1636 1833 1875  ../happy-whale/input/cropped_test_images/cropp...   \n",
       "2     787 787 1541 948  ../happy-whale/input/cropped_test_images/cropp...   \n",
       "3      77 228 2029 714  ../happy-whale/input/cropped_test_images/cropp...   \n",
       "4         4 20 289 317  ../happy-whale/input/cropped_test_images/cropp...   \n",
       "\n",
       "   individual_id  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Test Data\n",
    "test_df = pd.read_csv(f'{DATA_DIR}test.csv')\n",
    "test_df['Id'] = test_df['image'].apply(lambda x: f'{TEST_DIR}{x}')\n",
    "test_df['individual_id'] = 0\n",
    "\n",
    "# Summary\n",
    "print(f'test_df: {test_df.shape}')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMIItlTxLb-J"
   },
   "source": [
    "# MobileViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iyi916qOLb-J"
   },
   "source": [
    "MobileViT architecture comprised of following blocks\n",
    "* Strided 3x3 convolutions that process the input image\n",
    "* MobileNetV2-style inverted residual blocks for downsampling the resolution of the intermediate feature maps\n",
    "* MobileViT blocks that combine the benefits of Transformers and convolutions. It is presented in the figure below (taken from the original paper):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTPph4zWLb-J"
   },
   "source": [
    "![Screenshot from 2022-03-16 22-52-00.png](attachment:90baadc6-81f6-440f-8136-b0289236341f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.832776Z",
     "iopub.status.busy": "2022-03-17T09:07:13.832319Z",
     "iopub.status.idle": "2022-03-17T09:07:13.836608Z",
     "shell.execute_reply": "2022-03-17T09:07:13.835832Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.832738Z"
    },
    "id": "uQAsGaWHLb-J"
   },
   "outputs": [],
   "source": [
    "# Values are from table 4.\n",
    "patch_size = 4  # 2x2, for the Transformer blocks.\n",
    "image_size = 256\n",
    "expansion_factor = 2  # expansion factor for the MobileNetV2 blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.838761Z",
     "iopub.status.busy": "2022-03-17T09:07:13.838173Z",
     "iopub.status.idle": "2022-03-17T09:07:13.859257Z",
     "shell.execute_reply": "2022-03-17T09:07:13.858481Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.838709Z"
    },
    "id": "lQ5KYFQ_Lb-J"
   },
   "outputs": [],
   "source": [
    "def conv_block(x, filters=16, kernel_size=3, strides=2):\n",
    "    conv_layer = layers.Conv2D(\n",
    "        filters, kernel_size, strides=strides, activation=tf.nn.swish, padding=\"same\"\n",
    "    )\n",
    "    return conv_layer(x)\n",
    "\n",
    "\n",
    "# Reference: https://git.io/JKgtC\n",
    "\n",
    "\n",
    "def inverted_residual_block(x, expanded_channels, output_channels, strides=1):\n",
    "    m = layers.Conv2D(expanded_channels, 1, padding=\"same\", use_bias=False)(x)\n",
    "    m = layers.BatchNormalization()(m)\n",
    "    m = tf.nn.swish(m)\n",
    "\n",
    "    if strides == 2:\n",
    "        m = layers.ZeroPadding2D(padding=imagenet_utils.correct_pad(m, 3))(m)\n",
    "    m = layers.DepthwiseConv2D(\n",
    "        3, strides=strides, padding=\"same\" if strides == 1 else \"valid\", use_bias=False\n",
    "    )(m)\n",
    "    m = layers.BatchNormalization()(m)\n",
    "    m = tf.nn.swish(m)\n",
    "\n",
    "    m = layers.Conv2D(output_channels, 1, padding=\"same\", use_bias=False)(m)\n",
    "    m = layers.BatchNormalization()(m)\n",
    "\n",
    "    if tf.math.equal(x.shape[-1], output_channels) and strides == 1:\n",
    "        return layers.Add()([m, x])\n",
    "    return m\n",
    "\n",
    "\n",
    "# Reference:\n",
    "# https://keras.io/examples/vision/image_classification_with_vision_transformer/\n",
    "\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.swish)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def transformer_block(x, transformer_layers, projection_dim, num_heads=2):\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, x])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=[x.shape[-1] * 2, x.shape[-1]], dropout_rate=0.1,)\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add()([x3, x2])\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def mobilevit_block(x, num_blocks, projection_dim, strides=1):\n",
    "    # Local projection with convolutions.\n",
    "    local_features = conv_block(x, filters=projection_dim, strides=strides)\n",
    "    local_features = conv_block(\n",
    "        local_features, filters=projection_dim, kernel_size=1, strides=strides\n",
    "    )\n",
    "\n",
    "    # Unfold into patches and then pass through Transformers.\n",
    "    num_patches = int((local_features.shape[1] * local_features.shape[2]) / patch_size)\n",
    "    non_overlapping_patches = layers.Reshape((patch_size, num_patches, projection_dim))(\n",
    "        local_features\n",
    "    )\n",
    "    global_features = transformer_block(\n",
    "        non_overlapping_patches, num_blocks, projection_dim\n",
    "    )\n",
    "\n",
    "    # Fold into conv-like feature-maps.\n",
    "    folded_feature_map = layers.Reshape((*local_features.shape[1:-1], projection_dim))(\n",
    "        global_features\n",
    "    )\n",
    "\n",
    "    # Apply point-wise conv -> concatenate with the input features.\n",
    "    folded_feature_map = conv_block(\n",
    "        folded_feature_map, filters=x.shape[-1], kernel_size=1, strides=strides\n",
    "    )\n",
    "    local_global_features = layers.Concatenate(axis=-1)([x, folded_feature_map])\n",
    "\n",
    "    # Fuse the local and global features using a convoluion layer.\n",
    "    local_global_features = conv_block(\n",
    "        local_global_features, filters=projection_dim, strides=strides\n",
    "    )\n",
    "\n",
    "    return local_global_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79U9OV3XLb-K"
   },
   "source": [
    "# More on the MobileViT block:\n",
    "\n",
    "*     First, the feature representations (A) go through convolution blocks that capture local relationships. The expected shape of a single entry here would be (h, w, num_channels).\n",
    "*     Then they get unfolded into another vector with shape (p, n, num_channels), where p is the area of a small patch, and n is (h * w) / p. So, we end up with n non-overlapping patches.\n",
    "*     This unfolded vector is then passed through a Tranformer block that captures global relationships between the patches.\n",
    "*     The output vector (B) is again folded into a vector of shape (h, w, num_channels) resembling a feature map coming out of convolutions.\n",
    "\n",
    "Vectors A and B are then passed through two more convolutional layers to fuse the local and global representations. Notice how the spatial resolution of the final vector remains unchanged at this point. The authors also present an explanation of how the MobileViT block resembles a convolution block of a CNN. For more details, please refer to the original paper.\n",
    "\n",
    "Next, we combine these blocks together and implement the MobileViT architecture (XXS variant). The following figure (taken from the original paper) presents a schematic representation of the architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owFTHv-nLb-K"
   },
   "source": [
    "![Screenshot from 2022-03-16 22-55-19.png](attachment:7e6962e8-de9f-425b-b4a8-4507d44827d1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.862178Z",
     "iopub.status.busy": "2022-03-17T09:07:13.861675Z",
     "iopub.status.idle": "2022-03-17T09:07:16.103322Z",
     "shell.execute_reply": "2022-03-17T09:07:16.102607Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.862144Z"
    },
    "id": "1lbk1oDTLb-K"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imagenet_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-20-58095da733bd>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m \u001B[0mmobilevit_xxs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_mobilevit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m \u001B[0;31m#mobilevit_xxs.summary()\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-20-58095da733bd>\u001B[0m in \u001B[0;36mcreate_mobilevit\u001B[0;34m(num_classes)\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0;31m# Downsampling with MV2 block.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m     x = inverted_residual_block(\n\u001B[0;32m---> 15\u001B[0;31m         \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexpanded_channels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m16\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mexpansion_factor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput_channels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m24\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstrides\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m     )\n\u001B[1;32m     17\u001B[0m     x = inverted_residual_block(\n",
      "\u001B[0;32m<ipython-input-19-5d8959df22bd>\u001B[0m in \u001B[0;36minverted_residual_block\u001B[0;34m(x, expanded_channels, output_channels, strides)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mstrides\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m         \u001B[0mm\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlayers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mZeroPadding2D\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpadding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mimagenet_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcorrect_pad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m     m = layers.DepthwiseConv2D(\n\u001B[1;32m     19\u001B[0m         \u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstrides\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstrides\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpadding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"same\"\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mstrides\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m\"valid\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0muse_bias\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'imagenet_utils' is not defined"
     ]
    }
   ],
   "source": [
    "def create_mobilevit(num_classes=N_CLASSES):\n",
    "    inputs = keras.Input((image_size, image_size, 3))\n",
    "    \n",
    "    #version 2.7 tf x = layers.Rescaling(scale=1.0 / 255)(inputs)\n",
    "    x = tf.keras.layers.experimental.preprocessing.Rescaling(scale=1.0 / 255)(inputs)\n",
    "\n",
    "    # Initial conv-stem -> MV2 block.\n",
    "    x = conv_block(x, filters=16)\n",
    "    x = inverted_residual_block(\n",
    "        x, expanded_channels=16 * expansion_factor, output_channels=16\n",
    "    )\n",
    "\n",
    "    # Downsampling with MV2 block.\n",
    "    x = inverted_residual_block(\n",
    "        x, expanded_channels=16 * expansion_factor, output_channels=24, strides=2\n",
    "    )\n",
    "    x = inverted_residual_block(\n",
    "        x, expanded_channels=24 * expansion_factor, output_channels=24\n",
    "    )\n",
    "    x = inverted_residual_block(\n",
    "        x, expanded_channels=24 * expansion_factor, output_channels=24\n",
    "    )\n",
    "\n",
    "    # First MV2 -> MobileViT block.\n",
    "    x = inverted_residual_block(\n",
    "        x, expanded_channels=24 * expansion_factor, output_channels=48, strides=2\n",
    "    )\n",
    "    x = mobilevit_block(x, num_blocks=2, projection_dim=64)\n",
    "\n",
    "    # Second MV2 -> MobileViT block.\n",
    "    x = inverted_residual_block(\n",
    "        x, expanded_channels=64 * expansion_factor, output_channels=64, strides=2\n",
    "    )\n",
    "    x = mobilevit_block(x, num_blocks=4, projection_dim=80)\n",
    "\n",
    "    # Third MV2 -> MobileViT block.\n",
    "    x = inverted_residual_block(\n",
    "        x, expanded_channels=80 * expansion_factor, output_channels=80, strides=2\n",
    "    )\n",
    "    x = mobilevit_block(x, num_blocks=3, projection_dim=96)\n",
    "    x = conv_block(x, filters=320, kernel_size=1, strides=1)\n",
    "\n",
    "    # Classification head.\n",
    "    x = layers.GlobalAvgPool2D()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes,kernel_regularizer='l2', activation=\"softmax\")(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "mobilevit_xxs = create_mobilevit()\n",
    "#mobilevit_xxs.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:16.104947Z",
     "iopub.status.busy": "2022-03-17T09:07:16.104708Z",
     "iopub.status.idle": "2022-03-17T09:07:16.191901Z",
     "shell.execute_reply": "2022-03-17T09:07:16.191200Z",
     "shell.execute_reply.started": "2022-03-17T09:07:16.104913Z"
    },
    "id": "fR-JbMEkLb-K",
    "outputId": "c398f8cf-6dc1-4a45-d3f8-89e17942f759"
   },
   "outputs": [],
   "source": [
    "mobilevit_xxs.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:16.195179Z",
     "iopub.status.busy": "2022-03-17T09:07:16.194983Z",
     "iopub.status.idle": "2022-03-17T09:07:16.198620Z",
     "shell.execute_reply": "2022-03-17T09:07:16.197873Z",
     "shell.execute_reply.started": "2022-03-17T09:07:16.195154Z"
    },
    "id": "uYviyB-CLb-K"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "auto = tf.data.AUTOTUNE\n",
    "resize_bigger = 280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:16.200428Z",
     "iopub.status.busy": "2022-03-17T09:07:16.200143Z",
     "iopub.status.idle": "2022-03-17T09:07:16.210255Z",
     "shell.execute_reply": "2022-03-17T09:07:16.209586Z",
     "shell.execute_reply.started": "2022-03-17T09:07:16.200343Z"
    },
    "id": "i6QHmJVrLb-K"
   },
   "outputs": [],
   "source": [
    "individual_id_group = train_df['target_value'].copy().to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwsduPAfLb-L"
   },
   "source": [
    "# Training , Valdation Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:16.212082Z",
     "iopub.status.busy": "2022-03-17T09:07:16.211817Z",
     "iopub.status.idle": "2022-03-17T09:07:16.233401Z",
     "shell.execute_reply": "2022-03-17T09:07:16.232742Z",
     "shell.execute_reply.started": "2022-03-17T09:07:16.212048Z"
    },
    "id": "35nI5GmkLb-L"
   },
   "outputs": [],
   "source": [
    "trn_ds, val_ds = train_test_split(train_df,test_size=0.33, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:16.235164Z",
     "iopub.status.busy": "2022-03-17T09:07:16.234708Z",
     "iopub.status.idle": "2022-03-17T09:07:16.241858Z",
     "shell.execute_reply": "2022-03-17T09:07:16.240406Z",
     "shell.execute_reply.started": "2022-03-17T09:07:16.235128Z"
    },
    "id": "2l0v30B_Lb-L",
    "outputId": "3ad63bbf-c99b-48cd-fb6c-98e9d2bce7c8"
   },
   "outputs": [],
   "source": [
    "#train_df =  train_df.sample(frac = 0.1)\n",
    "trn_ds.shape, val_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:16.245698Z",
     "iopub.status.busy": "2022-03-17T09:07:16.243666Z",
     "iopub.status.idle": "2022-03-17T09:07:16.501789Z",
     "shell.execute_reply": "2022-03-17T09:07:16.501036Z",
     "shell.execute_reply.started": "2022-03-17T09:07:16.245665Z"
    },
    "id": "HbvwqNKCLb-L"
   },
   "outputs": [],
   "source": [
    "training_dataset = create_dataset(trn_ds,\n",
    "                                  batch_size  = BATCH_SIZE, \n",
    "                                  is_labelled = True, \n",
    "                                  augment = True,\n",
    "                                  repeat  = False, \n",
    "                                  shuffle = False,\n",
    "                                 size = (image_size, image_size))\n",
    "#sample_train_images, _ = next(iter(training_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:16.503370Z",
     "iopub.status.busy": "2022-03-17T09:07:16.503136Z",
     "iopub.status.idle": "2022-03-17T09:07:17.427598Z",
     "shell.execute_reply": "2022-03-17T09:07:17.426811Z",
     "shell.execute_reply.started": "2022-03-17T09:07:16.503335Z"
    },
    "id": "M3FtMGHULb-L"
   },
   "outputs": [],
   "source": [
    "batch, labels = next(iter(training_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:17.429824Z",
     "iopub.status.busy": "2022-03-17T09:07:17.429553Z",
     "iopub.status.idle": "2022-03-17T09:07:18.692179Z",
     "shell.execute_reply": "2022-03-17T09:07:18.691537Z",
     "shell.execute_reply.started": "2022-03-17T09:07:17.429790Z"
    },
    "id": "eUQRWNyTLb-L",
    "outputId": "b513cfbd-b9ee-4a3d-e456-f8b33bc3a930"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "for i, image in enumerate(batch[:20]):\n",
    "    ax = plt.subplot(5, 4, i + 1)\n",
    "    plt.title(labels[i].numpy())\n",
    "    plt.imshow(image.numpy().squeeze())\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:18.693327Z",
     "iopub.status.busy": "2022-03-17T09:07:18.693106Z",
     "iopub.status.idle": "2022-03-17T09:07:18.759142Z",
     "shell.execute_reply": "2022-03-17T09:07:18.758494Z",
     "shell.execute_reply.started": "2022-03-17T09:07:18.693297Z"
    },
    "id": "8kL9s3EiLb-M"
   },
   "outputs": [],
   "source": [
    "val_dataset = create_dataset(val_ds,\n",
    "                                  batch_size  = BATCH_SIZE, \n",
    "                                  is_labelled = True, \n",
    "                                  augment = True,\n",
    "                                  repeat  = False, \n",
    "                                  shuffle = False,\n",
    "                            size = (image_size, image_size))\n",
    "#sample_val_images, _ = next(iter(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:18.760766Z",
     "iopub.status.busy": "2022-03-17T09:07:18.760486Z",
     "iopub.status.idle": "2022-03-17T09:07:19.557275Z",
     "shell.execute_reply": "2022-03-17T09:07:19.556477Z",
     "shell.execute_reply.started": "2022-03-17T09:07:18.760729Z"
    },
    "id": "xgVBnO4FLb-M"
   },
   "outputs": [],
   "source": [
    "batch, labels = next(iter(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:19.559248Z",
     "iopub.status.busy": "2022-03-17T09:07:19.558996Z",
     "iopub.status.idle": "2022-03-17T09:07:21.239070Z",
     "shell.execute_reply": "2022-03-17T09:07:21.238436Z",
     "shell.execute_reply.started": "2022-03-17T09:07:19.559212Z"
    },
    "id": "hzhRLi28Lb-M",
    "outputId": "b4a04304-a6d0-419c-e15d-4d9ec6705edf"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "for i, image in enumerate(batch[:20]):\n",
    "    ax = plt.subplot(5, 4,  i + 1)\n",
    "    plt.title(labels[i].numpy())\n",
    "    plt.imshow(image.numpy().squeeze())\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sip1_9rALb-M"
   },
   "source": [
    "# Train a MobileViT (XXS) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T11:45:42.842571Z",
     "iopub.status.busy": "2022-03-17T11:45:42.841935Z",
     "iopub.status.idle": "2022-03-17T11:45:42.882255Z",
     "shell.execute_reply": "2022-03-17T11:45:42.881537Z",
     "shell.execute_reply.started": "2022-03-17T11:45:42.842509Z"
    },
    "id": "guPZnOO9Lb-M"
   },
   "outputs": [],
   "source": [
    "#unq_lbls = train_df.target_value.unique()\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "class_wgt = dict(zip(np.unique(train_df.target_value), class_weight.compute_class_weight(class_weight = 'balanced', classes = np.unique(train_df.target_value), \n",
    "                y = train_df.target_value))) \n",
    "#class_wgt = class_weight.compute_class_weight(class_weight = 'balanced',classes = np.unique(train_df.target_value),y = train_df.target_value )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T11:45:44.483398Z",
     "iopub.status.busy": "2022-03-17T11:45:44.483131Z",
     "iopub.status.idle": "2022-03-17T11:45:44.496600Z",
     "shell.execute_reply": "2022-03-17T11:45:44.495884Z",
     "shell.execute_reply.started": "2022-03-17T11:45:44.483365Z"
    },
    "id": "MhmFQrY9Lb-M"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.002\n",
    "label_smoothing_factor = 0.1\n",
    "epochs = 3\n",
    "\n",
    "#optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate,decay=1e-6, momentum=0.9, clipnorm=1. )\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "#loss_fn = keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing_factor)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "\n",
    "def run_experiment(model, fold, epochs=epochs):\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    if fold < 1 :\n",
    "        \n",
    "        model.compile(optimizer=optimizer, \n",
    "                              loss=loss_fn,\n",
    "                              #metrics = ['accuracy'])\n",
    "                              metrics = [tf.keras.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)])\n",
    "\n",
    "        checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "        checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "            checkpoint_filepath,\n",
    "            monitor=\"val_sparse_categorical_accuracy\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "        )\n",
    "\n",
    "\n",
    "        trn = train_df.iloc[train_index]\n",
    "        val = train_df.iloc[val_index]\n",
    "        training_dataset = create_dataset(trn, \n",
    "                                          batch_size  = BATCH_SIZE, \n",
    "                                          is_labelled = True, \n",
    "                                          augment     = True, \n",
    "                                          repeat      = False, \n",
    "                                          shuffle     = False,\n",
    "                                         size = (image_size, image_size))\n",
    "        val_dataset = create_dataset(val, \n",
    "                                            batch_size  = BATCH_SIZE, \n",
    "                                            is_labelled = True,\n",
    "                                            augment     = True, \n",
    "                                            repeat      = False,\n",
    "                                            shuffle     = False,\n",
    "                                           size = (image_size, image_size))\n",
    "\n",
    "\n",
    "        history = model.fit(\n",
    "            training_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=epochs,\n",
    "            class_weight = class_wgt,\n",
    "            callbacks=[checkpoint_callback],\n",
    "        )\n",
    "        #model.summary()\n",
    "        model.load_weights(checkpoint_filepath)\n",
    "        _,_, accuracy = model.evaluate(val_dataset)\n",
    "        print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFM-WRy7Lb-N"
   },
   "source": [
    "**We would like to see the images related to individual ids, so taken the samples base on  GroupKFold here.**\n",
    "\n",
    "*[Please Refer here for more inforamtion on GroupKfold](https://www.kaggle.com/reighns/groupkfold-and-stratified-groupkfold-efficientnet)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T11:45:47.457888Z",
     "iopub.status.busy": "2022-03-17T11:45:47.457178Z",
     "iopub.status.idle": "2022-03-17T12:52:55.901368Z",
     "shell.execute_reply": "2022-03-17T12:52:55.900608Z",
     "shell.execute_reply.started": "2022-03-17T11:45:47.457827Z"
    },
    "id": "WatfRnP7Lb-N",
    "outputId": "be4935e9-6acc-44aa-ad8c-bc2ddf7fa387"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "\n",
    "# OOF RMSE Placeholder\n",
    "\n",
    "kfold = GroupKFold(n_splits = NUM_FOLDS)\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(train_df.index, train_df['target_value'], groups=individual_id_group)):\n",
    "    mobilevit_xxs = create_mobilevit(num_classes=N_CLASSES)    \n",
    "    \n",
    "    run_experiment(mobilevit_xxs, fold)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-03-17T10:38:29.509478Z",
     "iopub.status.busy": "2022-03-17T10:38:29.508871Z",
     "iopub.status.idle": "2022-03-17T10:38:29.516029Z",
     "shell.execute_reply": "2022-03-17T10:38:29.515247Z",
     "shell.execute_reply.started": "2022-03-17T10:38:29.509436Z"
    },
    "id": "8CjwG_-8Lb-N",
    "outputId": "07925dd0-cb17-4237-e272-bd43443de63e"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "plt.figure(figsize=(8,8), tight_layout=True)\n",
    "plt.subplot(211)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(' Loss')\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('SparseCategoricalAccuracy Metric')\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:41:20.015324Z",
     "iopub.status.busy": "2022-03-17T09:41:20.014889Z",
     "iopub.status.idle": "2022-03-17T09:41:47.939925Z",
     "shell.execute_reply": "2022-03-17T09:41:47.939073Z",
     "shell.execute_reply.started": "2022-03-17T09:41:20.015289Z"
    },
    "id": "Zsr3uC__Lb-N",
    "outputId": "1ca95ab6-678f-405a-876f-48f78d850a9b"
   },
   "outputs": [],
   "source": [
    "# Serialize the model as a SavedModel.\n",
    "mobilevit_xxs.save(\"mobilevit_xxs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJnZrYIOLb-N"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T12:56:22.669803Z",
     "iopub.status.busy": "2022-03-17T12:56:22.669310Z",
     "iopub.status.idle": "2022-03-17T12:56:23.900957Z",
     "shell.execute_reply": "2022-03-17T12:56:23.899736Z",
     "shell.execute_reply.started": "2022-03-17T12:56:22.669764Z"
    },
    "id": "KLhyrY_ULb-N"
   },
   "outputs": [],
   "source": [
    "test_dataset = create_dataset(test_df,\n",
    "                                  batch_size  = BATCH_SIZE, \n",
    "                                  is_labelled = False, \n",
    "                                  augment = False,\n",
    "                                  repeat  = False, \n",
    "                                  shuffle = False,\n",
    "                            size = (image_size, image_size))\n",
    "test_images = next(iter(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T12:56:27.523496Z",
     "iopub.status.busy": "2022-03-17T12:56:27.523202Z",
     "iopub.status.idle": "2022-03-17T12:56:28.836327Z",
     "shell.execute_reply": "2022-03-17T12:56:28.835707Z",
     "shell.execute_reply.started": "2022-03-17T12:56:27.523462Z"
    },
    "id": "yNowc5_XLb-N",
    "outputId": "c4515c91-dced-43d7-a4be-f452a543b910"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "for i, image in enumerate(test_images[:20]):\n",
    "    ax = plt.subplot(5, 4,  i + 1)\n",
    "    #plt.title(\" Image\")\n",
    "    plt.imshow(image.numpy().squeeze())\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITz1nJvNLb-N"
   },
   "outputs": [],
   "source": [
    "#batch = next(iter(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-03-17T12:56:32.824786Z",
     "iopub.status.busy": "2022-03-17T12:56:32.823228Z",
     "iopub.status.idle": "2022-03-17T12:56:32.846158Z",
     "shell.execute_reply": "2022-03-17T12:56:32.845283Z",
     "shell.execute_reply.started": "2022-03-17T12:56:32.824731Z"
    },
    "id": "SNBhsvGCLb-N"
   },
   "outputs": [],
   "source": [
    "dict_ids =dict(zip( train_df['target_value'], train_df['individual_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T12:56:35.069093Z",
     "iopub.status.busy": "2022-03-17T12:56:35.068830Z",
     "iopub.status.idle": "2022-03-17T13:05:58.913741Z",
     "shell.execute_reply": "2022-03-17T13:05:58.912757Z",
     "shell.execute_reply.started": "2022-03-17T12:56:35.069061Z"
    },
    "id": "mVgPfog5Lb-O"
   },
   "outputs": [],
   "source": [
    "test_preds = mobilevit_xxs.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T10:17:13.116313Z",
     "iopub.status.busy": "2022-03-17T10:17:13.115494Z",
     "iopub.status.idle": "2022-03-17T10:17:13.122217Z",
     "shell.execute_reply": "2022-03-17T10:17:13.121389Z",
     "shell.execute_reply.started": "2022-03-17T10:17:13.116262Z"
    },
    "id": "o_IqTWeQLb-O",
    "outputId": "8cb0cd7a-b92b-4fdd-d179-b80b805aea32"
   },
   "outputs": [],
   "source": [
    "#dict_ids.get(1104)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-03-17T13:17:31.072783Z",
     "iopub.status.busy": "2022-03-17T13:17:31.072520Z",
     "iopub.status.idle": "2022-03-17T13:17:31.083058Z",
     "shell.execute_reply": "2022-03-17T13:17:31.082235Z",
     "shell.execute_reply.started": "2022-03-17T13:17:31.072749Z"
    },
    "id": "wPsOQ60BLb-O",
    "outputId": "2bfb2493-7f3f-4dc4-cc45-859331e27aaf"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "for i in range(2):\n",
    "    arr = test_preds[i].argsort()[-top_n:][::-1]\n",
    "    print (arr)\n",
    "    r  = [''.join(dict_ids.get(arr[j])) for j in range(top_n)]\n",
    "    print (r)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T13:18:22.004421Z",
     "iopub.status.busy": "2022-03-17T13:18:22.003619Z",
     "iopub.status.idle": "2022-03-17T13:19:09.846972Z",
     "shell.execute_reply": "2022-03-17T13:19:09.846147Z",
     "shell.execute_reply.started": "2022-03-17T13:18:22.004379Z"
    },
    "id": "x7OwB_71Lb-O",
    "outputId": "682ea7dd-2e1a-4796-c9fe-4e8c4c9eb3f2"
   },
   "outputs": [],
   "source": [
    "top_n = 5\n",
    "for i in range(len(test_preds)):\n",
    "    arr = test_preds[i].argsort()[-top_n:][::-1]\n",
    "    #print (arr)\n",
    "    test_df['predictions'][i]  = [''.join(dict_ids.get(arr[j])) for j in range(top_n)]\n",
    "    #print (r)\n",
    "    #test_df['predictions'][i] = r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T13:20:00.273448Z",
     "iopub.status.busy": "2022-03-17T13:20:00.272785Z",
     "iopub.status.idle": "2022-03-17T13:20:00.282131Z",
     "shell.execute_reply": "2022-03-17T13:20:00.281216Z",
     "shell.execute_reply.started": "2022-03-17T13:20:00.273406Z"
    },
    "id": "U1jgg_3YLb-O"
   },
   "outputs": [],
   "source": [
    "sub_df = test_df[['image', 'predictions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T13:20:08.360487Z",
     "iopub.status.busy": "2022-03-17T13:20:08.360198Z",
     "iopub.status.idle": "2022-03-17T13:20:08.513854Z",
     "shell.execute_reply": "2022-03-17T13:20:08.513060Z",
     "shell.execute_reply.started": "2022-03-17T13:20:08.360457Z"
    },
    "id": "9Aw4HWBwLb-O"
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T13:20:13.040614Z",
     "iopub.status.busy": "2022-03-17T13:20:13.040267Z",
     "iopub.status.idle": "2022-03-17T13:20:13.838153Z",
     "shell.execute_reply": "2022-03-17T13:20:13.837321Z",
     "shell.execute_reply.started": "2022-03-17T13:20:13.040576Z"
    },
    "id": "Z19jXNEWLb-O",
    "outputId": "85187b5e-4ab3-402c-cfa1-33126c80a3ef"
   },
   "outputs": [],
   "source": [
    "!tail submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNYP8zcGLb-O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JT9gFlOpLb-O"
   },
   "source": [
    "Provide your feedback to help me improve further. Thank you."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "tf-keras-mobilevit.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}