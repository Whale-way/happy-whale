{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8mZXfVsLb-D"
   },
   "source": [
    "# [Reference](https://keras.io/examples/vision/mobilevit/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHALHIZ4Lb-E"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this example, we implement the MobileViT architecture (Mehta et al.), which combines the benefits of Transformers (Vaswani et al.) and convolutions. With Transformers, we can capture long-range dependencies that result in global representations. With convolutions, we can capture spatial relationships that model locality.\n",
    "\n",
    "Besides combining the properties of Transformers and convolutions, the authors introduce MobileViT as a general-purpose mobile-friendly backbone for different image recognition tasks. Their findings suggest that, performance-wise, MobileViT is better than other models with the same or higher complexity (MobileNetV3, for example), while being efficient on mobile devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8X7oY7U6M0Z9",
    "outputId": "4b6373a3-d458-4ebd-a9ca-3a2973d736f5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "#allow to work on hertie server\n",
    "os.environ['TORCH_HOME']= '/workspace/cache'\n",
    "\n",
    "#!pip3 install tensorflow-addons\n",
    "#!pip3 install tensorflow-hub\n",
    "\n",
    "#!pip3 install --upgrade typing-extensions\n",
    "#!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:09.410745Z",
     "iopub.status.busy": "2022-03-17T09:07:09.410483Z",
     "iopub.status.idle": "2022-03-17T09:07:13.503386Z",
     "shell.execute_reply": "2022-03-17T09:07:13.502614Z",
     "shell.execute_reply.started": "2022-03-17T09:07:09.410678Z"
    },
    "id": "HVhAp0s6Lb-F",
    "outputId": "c1b6c7fe-8a97-49bb-d1b7-9df946bd256b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import os, cv2, gc\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras, Module \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow_hub as hub\n",
    "from keras.applications import imagenet_utils\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow_addons as tfa\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "try:\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    pass \n",
    "\n",
    "# enable mixed_precision and jit compiler \n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "tf.config.optimizer.set_jit(True)\n",
    "#allow gpu to work \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.505817Z",
     "iopub.status.busy": "2022-03-17T09:07:13.505527Z",
     "iopub.status.idle": "2022-03-17T09:07:13.512389Z",
     "shell.execute_reply": "2022-03-17T09:07:13.511717Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.505782Z"
    },
    "id": "K1cm2VNBLb-G"
   },
   "outputs": [],
   "source": [
    "INP_SIZE      = (512, 512) # Input size of the Image Resizer Module (IRM)\n",
    "TARGET_SIZE   = (224, 224) # Output size of IRM and Input size of the Vision Transformer \n",
    "INTERPOLATION = \"bilinear\"\n",
    "N_CLASSES = 15587\n",
    "#config_head = 'arcface'\n",
    "\n",
    "\n",
    "NUM_FOLDS  = 5\n",
    "BATCH_SIZE = 24\n",
    "SEED       = 42\n",
    "\n",
    "DATA_DIR  = '../happy-whale/input/'\n",
    "TRAIN_DIR = DATA_DIR + 'cropped_train_images/cropped_train_images/'\n",
    "TEST_DIR  = DATA_DIR + 'cropped_test_images/cropped_test_images/'\n",
    "\n",
    "# SetAutoTune\n",
    "AUTOTUNE = tf.data.AUTOTUNE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../happy-whale/input/cropped_train_images/cropped_train_images/'\n",
    "\n",
    "for file in os.listdir(path): \n",
    "    if not file.endswith(\".jpg\"): \n",
    "        img = Image.open(\"{path}/{file}\")\n",
    "        file_name, file_ext = os.path.splitext(file)\n",
    "        img.save('/jpg/{}.jpg'.format(file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjpFqD3eLb-G"
   },
   "source": [
    "# Prepare tf.data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.514318Z",
     "iopub.status.busy": "2022-03-17T09:07:13.513738Z",
     "iopub.status.idle": "2022-03-17T09:07:13.528971Z",
     "shell.execute_reply": "2022-03-17T09:07:13.528200Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.514281Z"
    },
    "id": "tU8L6ff_Lb-G"
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/ipythonx/tf-keras-learning-to-resize-image-for-vit-model/notebook\n",
    "def build_augmenter(is_labelled):\n",
    "    def augment(img):\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_flip_up_down(img)\n",
    "        img = tf.image.random_saturation(img, 0.65, 1.05)\n",
    "        img = tf.image.random_brightness(img, 0.05)\n",
    "        img = tf.image.random_contrast(img, 0.75, 1.05)\n",
    "        img = tf.image.random_hue(img, 0.05)\n",
    "        return img\n",
    "    \n",
    "    def augment_with_labels(img, label):\n",
    "        return augment(img), label\n",
    "    return augment_with_labels if is_labelled else augment\n",
    "\n",
    "def build_decoder(is_labelled, size):\n",
    "    def decode(path):\n",
    "        file_bytes = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(file_bytes, channels = 3)\n",
    "        img = tf.image.resize(img, (size[0], size[1]))\n",
    "        return tf.cast(tf.divide(img, 255.),tf.float32)\n",
    "    \n",
    "    def decode_with_labels(path, label):\n",
    "        label = tf.cast(label, tf.int32)\n",
    "        return decode(path),label\n",
    "    \n",
    "    return decode_with_labels if is_labelled else decode\n",
    "\n",
    "def create_dataset(df, \n",
    "                   batch_size  = 32, \n",
    "                   is_labelled = False, \n",
    "                   augment     = False, \n",
    "                   repeat      = False, \n",
    "                   shuffle     = False,\n",
    "                   size        = INP_SIZE):\n",
    "    decode_fn    = build_decoder(is_labelled, size)\n",
    "    augmenter_fn = build_augmenter(is_labelled)\n",
    "    \n",
    "    # Create Dataset\n",
    "    if is_labelled:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values, df['target_value'].values))\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values))\n",
    "        \n",
    "    dataset = dataset.map(decode_fn, num_parallel_calls = AUTOTUNE)\n",
    "    dataset = dataset.map(augmenter_fn, num_parallel_calls = AUTOTUNE) if augment else dataset\n",
    "    dataset = dataset.repeat() if repeat else dataset\n",
    "    dataset = dataset.shuffle(1024, reshuffle_each_iteration = True) if shuffle else dataset\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.532873Z",
     "iopub.status.busy": "2022-03-17T09:07:13.532462Z",
     "iopub.status.idle": "2022-03-17T09:07:13.540266Z",
     "shell.execute_reply": "2022-03-17T09:07:13.539401Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.532844Z"
    },
    "id": "uYlfV2w1Lb-H"
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.543268Z",
     "iopub.status.busy": "2022-03-17T09:07:13.542644Z",
     "iopub.status.idle": "2022-03-17T09:07:13.713068Z",
     "shell.execute_reply": "2022-03-17T09:07:13.712390Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.543230Z"
    },
    "id": "9IlEkxlMLb-H",
    "outputId": "518e3af4-2e8e-4e0c-a82d-904e5e5dfb9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df: (51033, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>box</th>\n",
       "      <th>Id</th>\n",
       "      <th>target_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>cadddb1636b9</td>\n",
       "      <td>2 116 802 665</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>12348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000562241d384d.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>1a71fbb72250</td>\n",
       "      <td>588 597 3504 1477</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007c33415ce37.jpg</td>\n",
       "      <td>false_killer_whale</td>\n",
       "      <td>60008f293a2b</td>\n",
       "      <td>0 453 3183 1589</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>5842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007d9bca26a99.jpg</td>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>4b00fe572063</td>\n",
       "      <td>1 91 2636 1495</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>4551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00087baf5cef7a.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>8e5253662392</td>\n",
       "      <td>1139 1590 3574 1913</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>8721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image             species individual_id                  box  \\\n",
       "0  00021adfb725ed.jpg  melon_headed_whale  cadddb1636b9        2 116 802 665   \n",
       "1  000562241d384d.jpg      humpback_whale  1a71fbb72250    588 597 3504 1477   \n",
       "2  0007c33415ce37.jpg  false_killer_whale  60008f293a2b      0 453 3183 1589   \n",
       "3  0007d9bca26a99.jpg  bottlenose_dolphin  4b00fe572063       1 91 2636 1495   \n",
       "4  00087baf5cef7a.jpg      humpback_whale  8e5253662392  1139 1590 3574 1913   \n",
       "\n",
       "                                                  Id  target_value  \n",
       "0  ../happy-whale/input/cropped_train_images/crop...         12348  \n",
       "1  ../happy-whale/input/cropped_train_images/crop...          1636  \n",
       "2  ../happy-whale/input/cropped_train_images/crop...          5842  \n",
       "3  ../happy-whale/input/cropped_train_images/crop...          4551  \n",
       "4  ../happy-whale/input/cropped_train_images/crop...          8721  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Train Data\n",
    "train_df = pd.read_csv(f'{DATA_DIR}train.csv')\n",
    "train_df['Id'] = train_df['image'].apply(lambda x: f'{TRAIN_DIR}{x}')\n",
    "\n",
    "# Adjust typos in \"species\" column from Andrada's kernel\n",
    "train_df[\"species\"] = train_df[\"species\"].replace([\"bottlenose_dolpin\", \"kiler_whale\",\n",
    "                                             \"beluga\", \n",
    "                                             \"globis\", \"pilot_whale\"],\n",
    "                                            [\"bottlenose_dolphin\", \"killer_whale\",\n",
    "                                             \"beluga_whale\", \n",
    "                                             \"short_finned_pilot_whale\", \"short_finned_pilot_whale\"])\n",
    "\n",
    "\n",
    "# Set a specific label to be able to perform stratification\n",
    "#train_df['stratify_label'] = train_df['individual_id']\n",
    "\n",
    "train_df['target_value']  = label_encoder.fit_transform(train_df['individual_id'] )\n",
    "\n",
    "# Summary\n",
    "print(f'train_df: {train_df.shape}')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T10:44:00.341686Z",
     "iopub.status.busy": "2022-03-17T10:44:00.341300Z",
     "iopub.status.idle": "2022-03-17T10:44:00.365251Z",
     "shell.execute_reply": "2022-03-17T10:44:00.364457Z",
     "shell.execute_reply.started": "2022-03-17T10:44:00.341645Z"
    },
    "id": "rEDSarr2Lb-I",
    "outputId": "ed9382ce-16f8-4947-f6a5-84cf809791ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>box</th>\n",
       "      <th>Id</th>\n",
       "      <th>target_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>001001f099519f.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>788 1036 2101 1373</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0164d8fd7a7cfe.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>676 640 2517 1144</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>01e775c518a824.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>12 641 2512 1425</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>02278d7a7d207e.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>1279 960 2630 1373</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>02d21d4df0b87f.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>2347 1222 2536 1368</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49109</th>\n",
       "      <td>f653df18636e4b.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>1778 1042 2308 1215</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49437</th>\n",
       "      <td>f8133d5fc02ba1.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>473 1060 2162 1511</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49879</th>\n",
       "      <td>fa2d984b021ec2.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>1485 1080 2194 1252</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50229</th>\n",
       "      <td>fbef59e7c8918c.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>1225 925 2910 1166</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50995</th>\n",
       "      <td>ffc71880c3066b.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>19fbb960f07d</td>\n",
       "      <td>2040 1397 2300 1585</td>\n",
       "      <td>../happy-whale/input/cropped_train_images/crop...</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    image      species individual_id                  box  \\\n",
       "10     001001f099519f.jpg  minke_whale  19fbb960f07d   788 1036 2101 1373   \n",
       "255    0164d8fd7a7cfe.jpg  minke_whale  19fbb960f07d    676 640 2517 1144   \n",
       "345    01e775c518a824.jpg  minke_whale  19fbb960f07d     12 641 2512 1425   \n",
       "393    02278d7a7d207e.jpg  minke_whale  19fbb960f07d   1279 960 2630 1373   \n",
       "528    02d21d4df0b87f.jpg  minke_whale  19fbb960f07d  2347 1222 2536 1368   \n",
       "...                   ...          ...           ...                  ...   \n",
       "49109  f653df18636e4b.jpg  minke_whale  19fbb960f07d  1778 1042 2308 1215   \n",
       "49437  f8133d5fc02ba1.jpg  minke_whale  19fbb960f07d   473 1060 2162 1511   \n",
       "49879  fa2d984b021ec2.jpg  minke_whale  19fbb960f07d  1485 1080 2194 1252   \n",
       "50229  fbef59e7c8918c.jpg  minke_whale  19fbb960f07d   1225 925 2910 1166   \n",
       "50995  ffc71880c3066b.jpg  minke_whale  19fbb960f07d  2040 1397 2300 1585   \n",
       "\n",
       "                                                      Id  target_value  \n",
       "10     ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "255    ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "345    ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "393    ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "528    ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "...                                                  ...           ...  \n",
       "49109  ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "49437  ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "49879  ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "50229  ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "50995  ../happy-whale/input/cropped_train_images/crop...          1613  \n",
       "\n",
       "[154 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['individual_id']=='19fbb960f07d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T10:42:12.903388Z",
     "iopub.status.busy": "2022-03-17T10:42:12.903048Z",
     "iopub.status.idle": "2022-03-17T10:42:12.924220Z",
     "shell.execute_reply": "2022-03-17T10:42:12.923342Z",
     "shell.execute_reply.started": "2022-03-17T10:42:12.903349Z"
    },
    "id": "1efHF-bvLb-I",
    "outputId": "7e31bcf8-88ee-4e84-b7c4-96dc2fd57658"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bottlenose_dolphin           10781\n",
       "beluga_whale                  7443\n",
       "humpback_whale                7392\n",
       "blue_whale                    4830\n",
       "false_killer_whale            3326\n",
       "dusky_dolphin                 3139\n",
       "killer_whale                  2455\n",
       "spinner_dolphin               1700\n",
       "melon_headed_whale            1689\n",
       "minke_whale                   1608\n",
       "fin_whale                     1324\n",
       "gray_whale                    1123\n",
       "southern_right_whale           866\n",
       "short_finned_pilot_whale       745\n",
       "spotted_dolphin                490\n",
       "sei_whale                      428\n",
       "common_dolphin                 347\n",
       "cuviers_beaked_whale           341\n",
       "long_finned_pilot_whale        238\n",
       "white_sided_dolphin            229\n",
       "brydes_whale                   154\n",
       "pantropic_spotted_dolphin      145\n",
       "commersons_dolphin              90\n",
       "pygmy_killer_whale              76\n",
       "rough_toothed_dolphin           60\n",
       "frasiers_dolphin                14\n",
       "Name: species, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.species.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.714990Z",
     "iopub.status.busy": "2022-03-17T09:07:13.714497Z",
     "iopub.status.idle": "2022-03-17T09:07:13.743074Z",
     "shell.execute_reply": "2022-03-17T09:07:13.742340Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.714952Z"
    },
    "id": "YVTJeEUnLb-I",
    "outputId": "e6d926b1-cac0-46fd-dfe7-89be159b97b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image                0\n",
       "species              0\n",
       "individual_id        0\n",
       "box              15362\n",
       "Id                   0\n",
       "target_value         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.746177Z",
     "iopub.status.busy": "2022-03-17T09:07:13.745933Z",
     "iopub.status.idle": "2022-03-17T09:07:13.752035Z",
     "shell.execute_reply": "2022-03-17T09:07:13.751288Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.746153Z"
    },
    "id": "TH1VzUwmLb-J"
   },
   "outputs": [],
   "source": [
    "train_ds, val_ds = train_df[:30000], train_df[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.753801Z",
     "iopub.status.busy": "2022-03-17T09:07:13.753336Z",
     "iopub.status.idle": "2022-03-17T09:07:13.830959Z",
     "shell.execute_reply": "2022-03-17T09:07:13.830251Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.753757Z"
    },
    "id": "qWmCYO7oLb-J",
    "outputId": "110e9d6a-9e54-4feb-92bb-5083628ada3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df: (27956, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>predictions</th>\n",
       "      <th>box</th>\n",
       "      <th>Id</th>\n",
       "      <th>individual_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000110707af0ba.jpg</td>\n",
       "      <td>37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...</td>\n",
       "      <td>329 874 2934 1271</td>\n",
       "      <td>../happy-whale/input/cropped_test_images/cropp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0006287ec424cb.jpg</td>\n",
       "      <td>37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...</td>\n",
       "      <td>1039 1636 1833 1875</td>\n",
       "      <td>../happy-whale/input/cropped_test_images/cropp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000809ecb2ccad.jpg</td>\n",
       "      <td>37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...</td>\n",
       "      <td>787 787 1541 948</td>\n",
       "      <td>../happy-whale/input/cropped_test_images/cropp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00098d1376dab2.jpg</td>\n",
       "      <td>37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...</td>\n",
       "      <td>77 228 2029 714</td>\n",
       "      <td>../happy-whale/input/cropped_test_images/cropp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000b8d89c738bd.jpg</td>\n",
       "      <td>37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...</td>\n",
       "      <td>4 20 289 317</td>\n",
       "      <td>../happy-whale/input/cropped_test_images/cropp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image                                        predictions  \\\n",
       "0  000110707af0ba.jpg  37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...   \n",
       "1  0006287ec424cb.jpg  37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...   \n",
       "2  000809ecb2ccad.jpg  37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...   \n",
       "3  00098d1376dab2.jpg  37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...   \n",
       "4  000b8d89c738bd.jpg  37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...   \n",
       "\n",
       "                   box                                                 Id  \\\n",
       "0    329 874 2934 1271  ../happy-whale/input/cropped_test_images/cropp...   \n",
       "1  1039 1636 1833 1875  ../happy-whale/input/cropped_test_images/cropp...   \n",
       "2     787 787 1541 948  ../happy-whale/input/cropped_test_images/cropp...   \n",
       "3      77 228 2029 714  ../happy-whale/input/cropped_test_images/cropp...   \n",
       "4         4 20 289 317  ../happy-whale/input/cropped_test_images/cropp...   \n",
       "\n",
       "   individual_id  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Test Data\n",
    "test_df = pd.read_csv(f'{DATA_DIR}test.csv')\n",
    "test_df['Id'] = test_df['image'].apply(lambda x: f'{TEST_DIR}{x}')\n",
    "test_df['individual_id'] = 0\n",
    "\n",
    "# Summary\n",
    "print(f'test_df: {test_df.shape}')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMIItlTxLb-J"
   },
   "source": [
    "# MobileViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iyi916qOLb-J"
   },
   "source": [
    "MobileViT architecture comprised of following blocks\n",
    "* Strided 3x3 convolutions that process the input image\n",
    "* MobileNetV2-style inverted residual blocks for downsampling the resolution of the intermediate feature maps\n",
    "* MobileViT blocks that combine the benefits of Transformers and convolutions. It is presented in the figure below (taken from the original paper):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTPph4zWLb-J"
   },
   "source": [
    "![Screenshot from 2022-03-16 22-52-00.png](attachment:90baadc6-81f6-440f-8136-b0289236341f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.832776Z",
     "iopub.status.busy": "2022-03-17T09:07:13.832319Z",
     "iopub.status.idle": "2022-03-17T09:07:13.836608Z",
     "shell.execute_reply": "2022-03-17T09:07:13.835832Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.832738Z"
    },
    "id": "uQAsGaWHLb-J"
   },
   "outputs": [],
   "source": [
    "# Values are from table 4.\n",
    "patch_size = 4  # 2x2, for the Transformer blocks.\n",
    "image_size = 256\n",
    "expansion_factor = 2  # expansion factor for the MobileNetV2 blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.838761Z",
     "iopub.status.busy": "2022-03-17T09:07:13.838173Z",
     "iopub.status.idle": "2022-03-17T09:07:13.859257Z",
     "shell.execute_reply": "2022-03-17T09:07:13.858481Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.838709Z"
    },
    "id": "lQ5KYFQ_Lb-J"
   },
   "outputs": [],
   "source": [
    "def conv_block(x, filters=16, kernel_size=3, strides=2):\n",
    "    conv_layer = layers.Conv2D(\n",
    "        filters, kernel_size, strides=strides, activation=tf.nn.swish, padding=\"same\"\n",
    "    )\n",
    "    return conv_layer(x)\n",
    "\n",
    "\n",
    "# Reference: https://git.io/JKgtC\n",
    "\n",
    "\n",
    "def inverted_residual_block(x, expanded_channels, output_channels, strides=1):\n",
    "    m = layers.Conv2D(expanded_channels, 1, padding=\"same\", use_bias=False)(x)\n",
    "    m = layers.BatchNormalization()(m)\n",
    "    m = tf.nn.swish(m)\n",
    "\n",
    "    if strides == 2:\n",
    "        m = layers.ZeroPadding2D(padding=imagenet_utils.correct_pad(m, 3))(m)\n",
    "    m = layers.DepthwiseConv2D(\n",
    "        3, strides=strides, padding=\"same\" if strides == 1 else \"valid\", use_bias=False\n",
    "    )(m)\n",
    "    m = layers.BatchNormalization()(m)\n",
    "    m = tf.nn.swish(m)\n",
    "\n",
    "    m = layers.Conv2D(output_channels, 1, padding=\"same\", use_bias=False)(m)\n",
    "    m = layers.BatchNormalization()(m)\n",
    "\n",
    "    if tf.math.equal(x.shape[-1], output_channels) and strides == 1:\n",
    "        return layers.Add()([m, x])\n",
    "    return m\n",
    "\n",
    "\n",
    "# Reference:\n",
    "# https://keras.io/examples/vision/image_classification_with_vision_transformer/\n",
    "\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.swish)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def transformer_block(x, transformer_layers, projection_dim, num_heads=2):\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, x])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=[x.shape[-1] * 2, x.shape[-1]], dropout_rate=0.1,)\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add()([x3, x2])\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def mobilevit_block(x, num_blocks, projection_dim, strides=1):\n",
    "    # Local projection with convolutions.\n",
    "    local_features = conv_block(x, filters=projection_dim, strides=strides)\n",
    "    local_features = conv_block(\n",
    "        local_features, filters=projection_dim, kernel_size=1, strides=strides\n",
    "    )\n",
    "\n",
    "    # Unfold into patches and then pass through Transformers.\n",
    "    num_patches = int((local_features.shape[1] * local_features.shape[2]) / patch_size)\n",
    "    non_overlapping_patches = layers.Reshape((patch_size, num_patches, projection_dim))(\n",
    "        local_features\n",
    "    )\n",
    "    global_features = transformer_block(\n",
    "        non_overlapping_patches, num_blocks, projection_dim\n",
    "    )\n",
    "\n",
    "    # Fold into conv-like feature-maps.\n",
    "    folded_feature_map = layers.Reshape((*local_features.shape[1:-1], projection_dim))(\n",
    "        global_features\n",
    "    )\n",
    "\n",
    "    # Apply point-wise conv -> concatenate with the input features.\n",
    "    folded_feature_map = conv_block(\n",
    "        folded_feature_map, filters=x.shape[-1], kernel_size=1, strides=strides\n",
    "    )\n",
    "    local_global_features = layers.Concatenate(axis=-1)([x, folded_feature_map])\n",
    "\n",
    "    # Fuse the local and global features using a convoluion layer.\n",
    "    local_global_features = conv_block(\n",
    "        local_global_features, filters=projection_dim, strides=strides\n",
    "    )\n",
    "\n",
    "    return local_global_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79U9OV3XLb-K"
   },
   "source": [
    "# More on the MobileViT block:\n",
    "\n",
    "*     First, the feature representations (A) go through convolution blocks that capture local relationships. The expected shape of a single entry here would be (h, w, num_channels).\n",
    "*     Then they get unfolded into another vector with shape (p, n, num_channels), where p is the area of a small patch, and n is (h * w) / p. So, we end up with n non-overlapping patches.\n",
    "*     This unfolded vector is then passed through a Tranformer block that captures global relationships between the patches.\n",
    "*     The output vector (B) is again folded into a vector of shape (h, w, num_channels) resembling a feature map coming out of convolutions.\n",
    "\n",
    "Vectors A and B are then passed through two more convolutional layers to fuse the local and global representations. Notice how the spatial resolution of the final vector remains unchanged at this point. The authors also present an explanation of how the MobileViT block resembles a convolution block of a CNN. For more details, please refer to the original paper.\n",
    "\n",
    "Next, we combine these blocks together and implement the MobileViT architecture (XXS variant). The following figure (taken from the original paper) presents a schematic representation of the architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owFTHv-nLb-K"
   },
   "source": [
    "![Screenshot from 2022-03-16 22-55-19.png](attachment:7e6962e8-de9f-425b-b4a8-4507d44827d1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:13.862178Z",
     "iopub.status.busy": "2022-03-17T09:07:13.861675Z",
     "iopub.status.idle": "2022-03-17T09:07:16.103322Z",
     "shell.execute_reply": "2022-03-17T09:07:16.102607Z",
     "shell.execute_reply.started": "2022-03-17T09:07:13.862144Z"
    },
    "id": "1lbk1oDTLb-K"
   },
   "outputs": [],
   "source": [
    "def create_mobilevit(num_classes=N_CLASSES):\n",
    "    inputs = keras.Input((image_size, image_size, 3))\n",
    "    \n",
    "    #version 2.7 tf x = layers.Rescaling(scale=1.0 / 255)(inputs)\n",
    "    x = tf.keras.layers.experimental.preprocessing.Rescaling(scale=1.0 / 255)(inputs)\n",
    "\n",
    "    # Initial conv-stem -> MV2 block.\n",
    "    x = conv_block(x, filters=16)\n",
    "    x = inverted_residual_block(\n",
    "        x, expanded_channels=16 * expansion_factor, output_channels=16\n",
    "    )\n",
    "\n",
    "    # Downsampling with MV2 block.\n",
    "    x = inverted_residual_block(\n",
    "        x, expanded_channels=16 * expansion_factor, output_channels=24, strides=2\n",
    "    )\n",
    "    x = inverted_residual_block(\n",
    "        x, expanded_channels=24 * expansion_factor, output_channels=24\n",
    "    )\n",
    "    x = inverted_residual_block(\n",
    "        x, expanded_channels=24 * expansion_factor, output_channels=24\n",
    "    )\n",
    "\n",
    "    # First MV2 -> MobileViT block.\n",
    "    x = inverted_residual_block(\n",
    "        x, expanded_channels=24 * expansion_factor, output_channels=48, strides=2\n",
    "    )\n",
    "    x = mobilevit_block(x, num_blocks=2, projection_dim=64)\n",
    "\n",
    "    # Second MV2 -> MobileViT block.\n",
    "    x = inverted_residual_block(\n",
    "        x, expanded_channels=64 * expansion_factor, output_channels=64, strides=2\n",
    "    )\n",
    "    x = mobilevit_block(x, num_blocks=4, projection_dim=80)\n",
    "\n",
    "    # Third MV2 -> MobileViT block.\n",
    "    x = inverted_residual_block(\n",
    "        x, expanded_channels=80 * expansion_factor, output_channels=80, strides=2\n",
    "    )\n",
    "    x = mobilevit_block(x, num_blocks=3, projection_dim=96)\n",
    "    x = conv_block(x, filters=320, kernel_size=1, strides=1)\n",
    "\n",
    "    # Classification head.\n",
    "    x = layers.GlobalAvgPool2D()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes,kernel_regularizer='l2', activation=\"softmax\")(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "mobilevit_xxs = create_mobilevit()\n",
    "#mobilevit_xxs.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:16.104947Z",
     "iopub.status.busy": "2022-03-17T09:07:16.104708Z",
     "iopub.status.idle": "2022-03-17T09:07:16.191901Z",
     "shell.execute_reply": "2022-03-17T09:07:16.191200Z",
     "shell.execute_reply.started": "2022-03-17T09:07:16.104913Z"
    },
    "id": "fR-JbMEkLb-K",
    "outputId": "c398f8cf-6dc1-4a45-d3f8-89e17942f759"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " rescaling (Rescaling)          (None, 256, 256, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 128, 128, 16  448         ['rescaling[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 128, 128, 32  512         ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 128, 128, 32  128        ['conv2d_1[0][0]']               \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " tf.nn.silu (TFOpLambda)        (None, 128, 128, 32  0           ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " depthwise_conv2d (DepthwiseCon  (None, 128, 128, 32  288        ['tf.nn.silu[0][0]']             \n",
      " v2D)                           )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 128, 128, 32  128        ['depthwise_conv2d[0][0]']       \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " tf.nn.silu_1 (TFOpLambda)      (None, 128, 128, 32  0           ['batch_normalization_1[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 128, 128, 16  512         ['tf.nn.silu_1[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 128, 128, 16  64         ['conv2d_2[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 128, 128, 16  0           ['batch_normalization_2[0][0]',  \n",
      "                                )                                 'conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 128, 128, 32  512         ['add[0][0]']                    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 128, 128, 32  128        ['conv2d_3[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " tf.nn.silu_2 (TFOpLambda)      (None, 128, 128, 32  0           ['batch_normalization_3[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " zero_padding2d (ZeroPadding2D)  (None, 129, 129, 32  0          ['tf.nn.silu_2[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " depthwise_conv2d_1 (DepthwiseC  (None, 64, 64, 32)  288         ['zero_padding2d[0][0]']         \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 64, 64, 32)  128         ['depthwise_conv2d_1[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.nn.silu_3 (TFOpLambda)      (None, 64, 64, 32)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 64, 64, 24)   768         ['tf.nn.silu_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 64, 64, 24)  96          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 64, 64, 48)   1152        ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 64, 64, 48)  192         ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.nn.silu_4 (TFOpLambda)      (None, 64, 64, 48)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " depthwise_conv2d_2 (DepthwiseC  (None, 64, 64, 48)  432         ['tf.nn.silu_4[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 64, 64, 48)  192         ['depthwise_conv2d_2[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.nn.silu_5 (TFOpLambda)      (None, 64, 64, 48)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 64, 64, 24)   1152        ['tf.nn.silu_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 64, 64, 24)  96          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " add_1 (Add)                    (None, 64, 64, 24)   0           ['batch_normalization_8[0][0]',  \n",
      "                                                                  'batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 64, 64, 48)   1152        ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 64, 64, 48)  192         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.nn.silu_6 (TFOpLambda)      (None, 64, 64, 48)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " depthwise_conv2d_3 (DepthwiseC  (None, 64, 64, 48)  432         ['tf.nn.silu_6[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 64, 64, 48)  192         ['depthwise_conv2d_3[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.nn.silu_7 (TFOpLambda)      (None, 64, 64, 48)   0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 64, 64, 24)   1152        ['tf.nn.silu_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 64, 64, 24)  96          ['conv2d_8[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 64, 64, 24)   0           ['batch_normalization_11[0][0]', \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 64, 64, 48)   1152        ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 64, 64, 48)  192         ['conv2d_9[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.nn.silu_8 (TFOpLambda)      (None, 64, 64, 48)   0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " zero_padding2d_1 (ZeroPadding2  (None, 65, 65, 48)  0           ['tf.nn.silu_8[0][0]']           \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " depthwise_conv2d_4 (DepthwiseC  (None, 32, 32, 48)  432         ['zero_padding2d_1[0][0]']       \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 32, 32, 48)  192         ['depthwise_conv2d_4[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.nn.silu_9 (TFOpLambda)      (None, 32, 32, 48)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 48)   2304        ['tf.nn.silu_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 32, 32, 48)  192         ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 64)   27712       ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 64)   4160        ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 4, 256, 64)   0           ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4, 256, 64)  128         ['reshape[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 4, 256, 64)  33216       ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4, 256, 64)   0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4, 256, 64)  128         ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4, 256, 128)  8320        ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4, 256, 128)  0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4, 256, 64)   8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4, 256, 64)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4, 256, 64)   0           ['dropout_1[0][0]',              \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4, 256, 64)  128         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 4, 256, 64)  33216       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4, 256, 64)   0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4, 256, 64)  128         ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4, 256, 128)  8320        ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4, 256, 128)  0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4, 256, 64)   8256        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4, 256, 64)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4, 256, 64)   0           ['dropout_3[0][0]',              \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 32, 32, 64)   0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 48)   3120        ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 32, 96)   0           ['batch_normalization_14[0][0]', \n",
      "                                                                  'conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 32, 32, 64)   55360       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 32, 32, 128)  8192        ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 32, 32, 128)  512        ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.nn.silu_10 (TFOpLambda)     (None, 32, 32, 128)  0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " zero_padding2d_2 (ZeroPadding2  (None, 33, 33, 128)  0          ['tf.nn.silu_10[0][0]']          \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " depthwise_conv2d_5 (DepthwiseC  (None, 16, 16, 128)  1152       ['zero_padding2d_2[0][0]']       \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 16, 16, 128)  512        ['depthwise_conv2d_5[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.nn.silu_11 (TFOpLambda)     (None, 16, 16, 128)  0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 16, 16, 64)   8192        ['tf.nn.silu_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 16, 16, 64)  256         ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 16, 16, 80)   46160       ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 16, 16, 80)   6480        ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 4, 64, 80)    0           ['conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4, 64, 80)   160         ['reshape_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 4, 64, 80)   51760       ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4, 64, 80)    0           ['multi_head_attention_2[0][0]', \n",
      "                                                                  'reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4, 64, 80)   160         ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4, 64, 160)   12960       ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4, 64, 160)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4, 64, 80)    12880       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4, 64, 80)    0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4, 64, 80)    0           ['dropout_5[0][0]',              \n",
      "                                                                  'add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4, 64, 80)   160         ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 4, 64, 80)   51760       ['layer_normalization_6[0][0]',  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4, 64, 80)    0           ['multi_head_attention_3[0][0]', \n",
      "                                                                  'add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4, 64, 80)   160         ['add_9[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4, 64, 160)   12960       ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4, 64, 160)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4, 64, 80)    12880       ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4, 64, 80)    0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4, 64, 80)    0           ['dropout_7[0][0]',              \n",
      "                                                                  'add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4, 64, 80)   160         ['add_10[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 4, 64, 80)   51760       ['layer_normalization_8[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4, 64, 80)    0           ['multi_head_attention_4[0][0]', \n",
      "                                                                  'add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4, 64, 80)   160         ['add_11[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4, 64, 160)   12960       ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4, 64, 160)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4, 64, 80)    12880       ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4, 64, 80)    0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4, 64, 80)    0           ['dropout_9[0][0]',              \n",
      "                                                                  'add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4, 64, 80)   160         ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 4, 64, 80)   51760       ['layer_normalization_10[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4, 64, 80)    0           ['multi_head_attention_5[0][0]', \n",
      "                                                                  'add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4, 64, 80)   160         ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4, 64, 160)   12960       ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4, 64, 160)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4, 64, 80)    12880       ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4, 64, 80)    0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4, 64, 80)    0           ['dropout_11[0][0]',             \n",
      "                                                                  'add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 16, 16, 80)   0           ['add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 16, 16, 64)   5184        ['reshape_3[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 16, 16, 128)  0           ['batch_normalization_17[0][0]', \n",
      "                                                                  'conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 16, 16, 80)   92240       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 16, 16, 160)  12800       ['conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 16, 16, 160)  640        ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.nn.silu_12 (TFOpLambda)     (None, 16, 16, 160)  0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " zero_padding2d_3 (ZeroPadding2  (None, 17, 17, 160)  0          ['tf.nn.silu_12[0][0]']          \n",
      " D)                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " depthwise_conv2d_6 (DepthwiseC  (None, 8, 8, 160)   1440        ['zero_padding2d_3[0][0]']       \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 8, 8, 160)   640         ['depthwise_conv2d_6[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.nn.silu_13 (TFOpLambda)     (None, 8, 8, 160)    0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 8, 8, 80)     12800       ['tf.nn.silu_13[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 8, 8, 80)    320         ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 8, 8, 96)     69216       ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 8, 8, 96)     9312        ['conv2d_23[0][0]']              \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 4, 16, 96)    0           ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4, 16, 96)   192         ['reshape_4[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (MultiH  (None, 4, 16, 96)   74400       ['layer_normalization_12[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4, 16, 96)    0           ['multi_head_attention_6[0][0]', \n",
      "                                                                  'reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4, 16, 96)   192         ['add_15[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4, 16, 192)   18624       ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4, 16, 192)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4, 16, 96)    18528       ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4, 16, 96)    0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 4, 16, 96)    0           ['dropout_13[0][0]',             \n",
      "                                                                  'add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4, 16, 96)   192         ['add_16[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (MultiH  (None, 4, 16, 96)   74400       ['layer_normalization_14[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 4, 16, 96)    0           ['multi_head_attention_7[0][0]', \n",
      "                                                                  'add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4, 16, 96)   192         ['add_17[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4, 16, 192)   18624       ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4, 16, 192)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4, 16, 96)    18528       ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4, 16, 96)    0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 4, 16, 96)    0           ['dropout_15[0][0]',             \n",
      "                                                                  'add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_16 (LayerN  (None, 4, 16, 96)   192         ['add_18[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_8 (MultiH  (None, 4, 16, 96)   74400       ['layer_normalization_16[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 4, 16, 96)    0           ['multi_head_attention_8[0][0]', \n",
      "                                                                  'add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_17 (LayerN  (None, 4, 16, 96)   192         ['add_19[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4, 16, 192)   18624       ['layer_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4, 16, 192)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 4, 16, 96)    18528       ['dropout_16[0][0]']             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4, 16, 96)    0           ['dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 4, 16, 96)    0           ['dropout_17[0][0]',             \n",
      "                                                                  'add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, 8, 8, 96)     0           ['add_20[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 8, 8, 80)     7760        ['reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 8, 8, 160)    0           ['batch_normalization_20[0][0]', \n",
      "                                                                  'conv2d_25[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 8, 8, 96)     138336      ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 8, 8, 320)    31040       ['conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 320)         0           ['conv2d_27[0][0]']              \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 320)          0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 15587)        5003427     ['dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,309,443\n",
      "Trainable params: 6,306,899\n",
      "Non-trainable params: 2,544\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mobilevit_xxs.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:16.195179Z",
     "iopub.status.busy": "2022-03-17T09:07:16.194983Z",
     "iopub.status.idle": "2022-03-17T09:07:16.198620Z",
     "shell.execute_reply": "2022-03-17T09:07:16.197873Z",
     "shell.execute_reply.started": "2022-03-17T09:07:16.195154Z"
    },
    "id": "uYviyB-CLb-K"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "auto = tf.data.AUTOTUNE\n",
    "resize_bigger = 280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:16.200428Z",
     "iopub.status.busy": "2022-03-17T09:07:16.200143Z",
     "iopub.status.idle": "2022-03-17T09:07:16.210255Z",
     "shell.execute_reply": "2022-03-17T09:07:16.209586Z",
     "shell.execute_reply.started": "2022-03-17T09:07:16.200343Z"
    },
    "id": "i6QHmJVrLb-K"
   },
   "outputs": [],
   "source": [
    "individual_id_group = train_df['target_value'].copy().to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwsduPAfLb-L"
   },
   "source": [
    "# Training , Valdation Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:16.212082Z",
     "iopub.status.busy": "2022-03-17T09:07:16.211817Z",
     "iopub.status.idle": "2022-03-17T09:07:16.233401Z",
     "shell.execute_reply": "2022-03-17T09:07:16.232742Z",
     "shell.execute_reply.started": "2022-03-17T09:07:16.212048Z"
    },
    "id": "35nI5GmkLb-L"
   },
   "outputs": [],
   "source": [
    "trn_ds, val_ds = train_test_split(train_df,test_size=0.33, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:16.235164Z",
     "iopub.status.busy": "2022-03-17T09:07:16.234708Z",
     "iopub.status.idle": "2022-03-17T09:07:16.241858Z",
     "shell.execute_reply": "2022-03-17T09:07:16.240406Z",
     "shell.execute_reply.started": "2022-03-17T09:07:16.235128Z"
    },
    "id": "2l0v30B_Lb-L",
    "outputId": "3ad63bbf-c99b-48cd-fb6c-98e9d2bce7c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34192, 6), (16841, 6))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df =  train_df.sample(frac = 0.1)\n",
    "trn_ds.shape, val_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:16.245698Z",
     "iopub.status.busy": "2022-03-17T09:07:16.243666Z",
     "iopub.status.idle": "2022-03-17T09:07:16.501789Z",
     "shell.execute_reply": "2022-03-17T09:07:16.501036Z",
     "shell.execute_reply.started": "2022-03-17T09:07:16.245665Z"
    },
    "id": "HbvwqNKCLb-L"
   },
   "outputs": [],
   "source": [
    "training_dataset = create_dataset(trn_ds,\n",
    "                                  batch_size  = BATCH_SIZE, \n",
    "                                  is_labelled = True, \n",
    "                                  augment = True,\n",
    "                                  repeat  = False, \n",
    "                                  shuffle = False,\n",
    "                                 size = (image_size, image_size))\n",
    "#sample_train_images, _ = next(iter(training_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:16.503370Z",
     "iopub.status.busy": "2022-03-17T09:07:16.503136Z",
     "iopub.status.idle": "2022-03-17T09:07:17.427598Z",
     "shell.execute_reply": "2022-03-17T09:07:17.426811Z",
     "shell.execute_reply.started": "2022-03-17T09:07:16.503335Z"
    },
    "id": "M3FtMGHULb-L"
   },
   "outputs": [],
   "source": [
    "batch, labels = next(iter(training_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:17.429824Z",
     "iopub.status.busy": "2022-03-17T09:07:17.429553Z",
     "iopub.status.idle": "2022-03-17T09:07:18.692179Z",
     "shell.execute_reply": "2022-03-17T09:07:18.691537Z",
     "shell.execute_reply.started": "2022-03-17T09:07:17.429790Z"
    },
    "id": "eUQRWNyTLb-L",
    "outputId": "b513cfbd-b9ee-4a3d-e456-f8b33bc3a930"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "for i, image in enumerate(batch[:20]):\n",
    "    ax = plt.subplot(5, 4, i + 1)\n",
    "    plt.title(labels[i].numpy())\n",
    "    plt.imshow(image.numpy().squeeze())\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:18.693327Z",
     "iopub.status.busy": "2022-03-17T09:07:18.693106Z",
     "iopub.status.idle": "2022-03-17T09:07:18.759142Z",
     "shell.execute_reply": "2022-03-17T09:07:18.758494Z",
     "shell.execute_reply.started": "2022-03-17T09:07:18.693297Z"
    },
    "id": "8kL9s3EiLb-M"
   },
   "outputs": [],
   "source": [
    "val_dataset = create_dataset(val_ds,\n",
    "                                  batch_size  = BATCH_SIZE, \n",
    "                                  is_labelled = True, \n",
    "                                  augment = True,\n",
    "                                  repeat  = False, \n",
    "                                  shuffle = False,\n",
    "                            size = (image_size, image_size))\n",
    "#sample_val_images, _ = next(iter(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:18.760766Z",
     "iopub.status.busy": "2022-03-17T09:07:18.760486Z",
     "iopub.status.idle": "2022-03-17T09:07:19.557275Z",
     "shell.execute_reply": "2022-03-17T09:07:19.556477Z",
     "shell.execute_reply.started": "2022-03-17T09:07:18.760729Z"
    },
    "id": "xgVBnO4FLb-M"
   },
   "outputs": [],
   "source": [
    "batch, labels = next(iter(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:07:19.559248Z",
     "iopub.status.busy": "2022-03-17T09:07:19.558996Z",
     "iopub.status.idle": "2022-03-17T09:07:21.239070Z",
     "shell.execute_reply": "2022-03-17T09:07:21.238436Z",
     "shell.execute_reply.started": "2022-03-17T09:07:19.559212Z"
    },
    "id": "hzhRLi28Lb-M",
    "outputId": "b4a04304-a6d0-419c-e15d-4d9ec6705edf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "for i, image in enumerate(batch[:20]):\n",
    "    ax = plt.subplot(5, 4,  i + 1)\n",
    "    plt.title(labels[i].numpy())\n",
    "    plt.imshow(image.numpy().squeeze())\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sip1_9rALb-M"
   },
   "source": [
    "# Train a MobileViT (XXS) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T11:45:42.842571Z",
     "iopub.status.busy": "2022-03-17T11:45:42.841935Z",
     "iopub.status.idle": "2022-03-17T11:45:42.882255Z",
     "shell.execute_reply": "2022-03-17T11:45:42.881537Z",
     "shell.execute_reply.started": "2022-03-17T11:45:42.842509Z"
    },
    "id": "guPZnOO9Lb-M"
   },
   "outputs": [],
   "source": [
    "#unq_lbls = train_df.target_value.unique()\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "class_wgt = dict(zip(np.unique(train_df.target_value), class_weight.compute_class_weight(class_weight = 'balanced', classes = np.unique(train_df.target_value), \n",
    "                y = train_df.target_value))) \n",
    "#class_wgt = class_weight.compute_class_weight(class_weight = 'balanced',classes = np.unique(train_df.target_value),y = train_df.target_value )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T11:45:44.483398Z",
     "iopub.status.busy": "2022-03-17T11:45:44.483131Z",
     "iopub.status.idle": "2022-03-17T11:45:44.496600Z",
     "shell.execute_reply": "2022-03-17T11:45:44.495884Z",
     "shell.execute_reply.started": "2022-03-17T11:45:44.483365Z"
    },
    "id": "MhmFQrY9Lb-M"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.002\n",
    "label_smoothing_factor = 0.1\n",
    "epochs = 3\n",
    "\n",
    "#optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate,decay=1e-6, momentum=0.9, clipnorm=1. )\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "#loss_fn = keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing_factor)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "\n",
    "def run_experiment(model, fold, epochs=epochs):\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    if fold < 1 :\n",
    "        \n",
    "        model.compile(optimizer=optimizer, \n",
    "                              loss=loss_fn,\n",
    "                              #metrics = ['accuracy'])\n",
    "                              metrics = [tf.keras.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)])\n",
    "\n",
    "        checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "        checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "            checkpoint_filepath,\n",
    "            monitor=\"val_sparse_categorical_accuracy\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "        )\n",
    "\n",
    "\n",
    "        trn = train_df.iloc[train_index]\n",
    "        val = train_df.iloc[val_index]\n",
    "        training_dataset = create_dataset(trn, \n",
    "                                          batch_size  = BATCH_SIZE, \n",
    "                                          is_labelled = True, \n",
    "                                          augment     = True, \n",
    "                                          repeat      = False, \n",
    "                                          shuffle     = False,\n",
    "                                         size = (image_size, image_size))\n",
    "        val_dataset = create_dataset(val, \n",
    "                                            batch_size  = BATCH_SIZE, \n",
    "                                            is_labelled = True,\n",
    "                                            augment     = True, \n",
    "                                            repeat      = False,\n",
    "                                            shuffle     = False,\n",
    "                                           size = (image_size, image_size))\n",
    "\n",
    "\n",
    "        history = model.fit(\n",
    "            training_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=epochs,\n",
    "            class_weight = class_wgt,\n",
    "            callbacks=[checkpoint_callback],\n",
    "        )\n",
    "        #model.summary()\n",
    "        model.load_weights(checkpoint_filepath)\n",
    "        _,_, accuracy = model.evaluate(val_dataset)\n",
    "        print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFM-WRy7Lb-N"
   },
   "source": [
    "**We would like to see the images related to individual ids, so taken the samples base on  GroupKFold here.**\n",
    "\n",
    "*[Please Refer here for more inforamtion on GroupKfold](https://www.kaggle.com/reighns/groupkfold-and-stratified-groupkfold-efficientnet)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T11:45:47.457888Z",
     "iopub.status.busy": "2022-03-17T11:45:47.457178Z",
     "iopub.status.idle": "2022-03-17T12:52:55.901368Z",
     "shell.execute_reply": "2022-03-17T12:52:55.900608Z",
     "shell.execute_reply.started": "2022-03-17T11:45:47.457827Z"
    },
    "id": "WatfRnP7Lb-N",
    "outputId": "be4935e9-6acc-44aa-ad8c-bc2ddf7fa387"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1702/1702 [==============================] - 235s 111ms/step - loss: nan - sparse_categorical_accuracy: 4.1640e-04 - sparse_top_k_categorical_accuracy: 0.0016 - val_loss: nan - val_sparse_categorical_accuracy: 0.0000e+00 - val_sparse_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "1702/1702 [==============================] - 195s 115ms/step - loss: nan - sparse_categorical_accuracy: 2.4494e-05 - sparse_top_k_categorical_accuracy: 0.0000e+00 - val_loss: nan - val_sparse_categorical_accuracy: 0.0000e+00 - val_sparse_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 3/3\n",
      "1702/1702 [==============================] - 169s 99ms/step - loss: nan - sparse_categorical_accuracy: 2.4494e-05 - sparse_top_k_categorical_accuracy: 0.0000e+00 - val_loss: nan - val_sparse_categorical_accuracy: 0.0000e+00 - val_sparse_top_k_categorical_accuracy: 0.0000e+00\n",
      "426/426 [==============================] - 9s 21ms/step - loss: nan - sparse_categorical_accuracy: 0.0000e+00 - sparse_top_k_categorical_accuracy: 0.0000e+00\n",
      "Validation accuracy: 0.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "\n",
    "# OOF RMSE Placeholder\n",
    "\n",
    "kfold = GroupKFold(n_splits = NUM_FOLDS)\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(train_df.index, train_df['target_value'], groups=individual_id_group)):\n",
    "    mobilevit_xxs = create_mobilevit(num_classes=N_CLASSES)    \n",
    "    \n",
    "    run_experiment(mobilevit_xxs, fold)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-03-17T10:38:29.509478Z",
     "iopub.status.busy": "2022-03-17T10:38:29.508871Z",
     "iopub.status.idle": "2022-03-17T10:38:29.516029Z",
     "shell.execute_reply": "2022-03-17T10:38:29.515247Z",
     "shell.execute_reply.started": "2022-03-17T10:38:29.509436Z"
    },
    "id": "8CjwG_-8Lb-N",
    "outputId": "07925dd0-cb17-4237-e272-bd43443de63e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.figure(figsize=(8,8), tight_layout=True)\\nplt.subplot(211)\\nplt.plot(history.history['loss'])\\nplt.plot(history.history['val_loss'])\\nplt.title(' Loss')\\nplt.ylabel('Error')\\nplt.xlabel('Epoch')\\nplt.legend(['train', 'val'], loc='upper right')\\n\\nplt.subplot(212)\\nplt.plot(history.history['sparse_categorical_accuracy'])\\nplt.plot(history.history['val_sparse_categorical_accuracy'])\\nplt.title('SparseCategoricalAccuracy Metric')\\nplt.ylabel('Error')\\nplt.xlabel('Epoch')\\nplt.legend(['train', 'val'], loc='upper left')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "plt.figure(figsize=(8,8), tight_layout=True)\n",
    "plt.subplot(211)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(' Loss')\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('SparseCategoricalAccuracy Metric')\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T09:41:20.015324Z",
     "iopub.status.busy": "2022-03-17T09:41:20.014889Z",
     "iopub.status.idle": "2022-03-17T09:41:47.939925Z",
     "shell.execute_reply": "2022-03-17T09:41:47.939073Z",
     "shell.execute_reply.started": "2022-03-17T09:41:20.015289Z"
    },
    "id": "Zsr3uC__Lb-N",
    "outputId": "1ca95ab6-678f-405a-876f-48f78d850a9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 108). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mobilevit_xxs/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mobilevit_xxs/assets\n"
     ]
    }
   ],
   "source": [
    "# Serialize the model as a SavedModel.\n",
    "mobilevit_xxs.save(\"mobilevit_xxs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJnZrYIOLb-N"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T12:56:22.669803Z",
     "iopub.status.busy": "2022-03-17T12:56:22.669310Z",
     "iopub.status.idle": "2022-03-17T12:56:23.900957Z",
     "shell.execute_reply": "2022-03-17T12:56:23.899736Z",
     "shell.execute_reply.started": "2022-03-17T12:56:22.669764Z"
    },
    "id": "KLhyrY_ULb-N"
   },
   "outputs": [],
   "source": [
    "test_dataset = create_dataset(test_df,\n",
    "                                  batch_size  = BATCH_SIZE, \n",
    "                                  is_labelled = False, \n",
    "                                  augment = False,\n",
    "                                  repeat  = False, \n",
    "                                  shuffle = False,\n",
    "                            size = (image_size, image_size))\n",
    "test_images = next(iter(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T12:56:27.523496Z",
     "iopub.status.busy": "2022-03-17T12:56:27.523202Z",
     "iopub.status.idle": "2022-03-17T12:56:28.836327Z",
     "shell.execute_reply": "2022-03-17T12:56:28.835707Z",
     "shell.execute_reply.started": "2022-03-17T12:56:27.523462Z"
    },
    "id": "yNowc5_XLb-N",
    "outputId": "c4515c91-dced-43d7-a4be-f452a543b910"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "for i, image in enumerate(test_images[:20]):\n",
    "    ax = plt.subplot(5, 4,  i + 1)\n",
    "    #plt.title(\" Image\")\n",
    "    plt.imshow(image.numpy().squeeze())\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ITz1nJvNLb-N"
   },
   "outputs": [],
   "source": [
    "#batch = next(iter(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-03-17T12:56:32.824786Z",
     "iopub.status.busy": "2022-03-17T12:56:32.823228Z",
     "iopub.status.idle": "2022-03-17T12:56:32.846158Z",
     "shell.execute_reply": "2022-03-17T12:56:32.845283Z",
     "shell.execute_reply.started": "2022-03-17T12:56:32.824731Z"
    },
    "id": "SNBhsvGCLb-N"
   },
   "outputs": [],
   "source": [
    "dict_ids =dict(zip( train_df['target_value'], train_df['individual_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T12:56:35.069093Z",
     "iopub.status.busy": "2022-03-17T12:56:35.068830Z",
     "iopub.status.idle": "2022-03-17T13:05:58.913741Z",
     "shell.execute_reply": "2022-03-17T13:05:58.912757Z",
     "shell.execute_reply.started": "2022-03-17T12:56:35.069061Z"
    },
    "id": "mVgPfog5Lb-O"
   },
   "outputs": [],
   "source": [
    "test_preds = mobilevit_xxs.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T10:17:13.116313Z",
     "iopub.status.busy": "2022-03-17T10:17:13.115494Z",
     "iopub.status.idle": "2022-03-17T10:17:13.122217Z",
     "shell.execute_reply": "2022-03-17T10:17:13.121389Z",
     "shell.execute_reply.started": "2022-03-17T10:17:13.116262Z"
    },
    "id": "o_IqTWeQLb-O",
    "outputId": "8cb0cd7a-b92b-4fdd-d179-b80b805aea32"
   },
   "outputs": [],
   "source": [
    "#dict_ids.get(1104)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-03-17T13:17:31.072783Z",
     "iopub.status.busy": "2022-03-17T13:17:31.072520Z",
     "iopub.status.idle": "2022-03-17T13:17:31.083058Z",
     "shell.execute_reply": "2022-03-17T13:17:31.082235Z",
     "shell.execute_reply.started": "2022-03-17T13:17:31.072749Z"
    },
    "id": "wPsOQ60BLb-O",
    "outputId": "2bfb2493-7f3f-4dc4-cc45-859331e27aaf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in range(2):\\n    arr = test_preds[i].argsort()[-top_n:][::-1]\\n    print (arr)\\n    r  = [''.join(dict_ids.get(arr[j])) for j in range(top_n)]\\n    print (r)\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for i in range(2):\n",
    "    arr = test_preds[i].argsort()[-top_n:][::-1]\n",
    "    print (arr)\n",
    "    r  = [''.join(dict_ids.get(arr[j])) for j in range(top_n)]\n",
    "    print (r)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T13:18:22.004421Z",
     "iopub.status.busy": "2022-03-17T13:18:22.003619Z",
     "iopub.status.idle": "2022-03-17T13:19:09.846972Z",
     "shell.execute_reply": "2022-03-17T13:19:09.846147Z",
     "shell.execute_reply.started": "2022-03-17T13:18:22.004379Z"
    },
    "id": "x7OwB_71Lb-O",
    "outputId": "682ea7dd-2e1a-4796-c9fe-4e8c4c9eb3f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-42-a8d11cfb969f>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['predictions'][i]  = [''.join(dict_ids.get(arr[j])) for j in range(top_n)]\n"
     ]
    }
   ],
   "source": [
    "top_n = 5\n",
    "for i in range(len(test_preds)):\n",
    "    arr = test_preds[i].argsort()[-top_n:][::-1]\n",
    "    #print (arr)\n",
    "    test_df['predictions'][i]  = [''.join(dict_ids.get(arr[j])) for j in range(top_n)]\n",
    "    #print (r)\n",
    "    #test_df['predictions'][i] = r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T13:20:00.273448Z",
     "iopub.status.busy": "2022-03-17T13:20:00.272785Z",
     "iopub.status.idle": "2022-03-17T13:20:00.282131Z",
     "shell.execute_reply": "2022-03-17T13:20:00.281216Z",
     "shell.execute_reply.started": "2022-03-17T13:20:00.273406Z"
    },
    "id": "U1jgg_3YLb-O"
   },
   "outputs": [],
   "source": [
    "sub_df = test_df[['image', 'predictions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T13:20:08.360487Z",
     "iopub.status.busy": "2022-03-17T13:20:08.360198Z",
     "iopub.status.idle": "2022-03-17T13:20:08.513854Z",
     "shell.execute_reply": "2022-03-17T13:20:08.513060Z",
     "shell.execute_reply.started": "2022-03-17T13:20:08.360457Z"
    },
    "id": "9Aw4HWBwLb-O"
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T13:20:13.040614Z",
     "iopub.status.busy": "2022-03-17T13:20:13.040267Z",
     "iopub.status.idle": "2022-03-17T13:20:13.838153Z",
     "shell.execute_reply": "2022-03-17T13:20:13.837321Z",
     "shell.execute_reply.started": "2022-03-17T13:20:13.040576Z"
    },
    "id": "Z19jXNEWLb-O",
    "outputId": "85187b5e-4ab3-402c-cfa1-33126c80a3ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffe1e7587409a4.jpg,\"['6ec0697b3511', 'd7b71850df75', '63688765c3c5', '0d1d09bc7782', 'b29550f1dc3b']\"\r\n",
      "fff024257c4246.jpg,\"['3a544f9150cd', '9267dd80fa4e', '0d1d09bc7782', '64e1844c65f9', 'b9502efd2ef2']\"\r\n",
      "fff0f68c5b4c4c.jpg,\"['d7b71850df75', '6ec0697b3511', '63688765c3c5', '0d1d09bc7782', 'b29550f1dc3b']\"\r\n",
      "fff2466b650dd7.jpg,\"['3a544f9150cd', '0d1d09bc7782', '9267dd80fa4e', 'a1faa057fbb4', '86c548ebb760']\"\r\n",
      "fff6153701bae2.jpg,\"['6ec0697b3511', 'd7b71850df75', '0d1d09bc7782', '63688765c3c5', 'b29550f1dc3b']\"\r\n",
      "fff6ff1989b5cd.jpg,\"['0d1d09bc7782', '6ec0697b3511', '63688765c3c5', '16c95e4c6d13', 'b29550f1dc3b']\"\r\n",
      "fff8fd932b42cb.jpg,\"['9267dd80fa4e', '0d1d09bc7782', '3a544f9150cd', '63688765c3c5', 'a1faa057fbb4']\"\r\n",
      "fff96371332c16.jpg,\"['3a544f9150cd', '9267dd80fa4e', '0d1d09bc7782', '64e1844c65f9', 'a1faa057fbb4']\"\r\n",
      "fffc1c4d3eabc7.jpg,\"['6ec0697b3511', '0d1d09bc7782', 'd7b71850df75', '63688765c3c5', 'b29550f1dc3b']\"\r\n",
      "fffc50be10c175.jpg,\"['0d1d09bc7782', '6ec0697b3511', '3a544f9150cd', '86c548ebb760', 'b29550f1dc3b']\"\r\n"
     ]
    }
   ],
   "source": [
    "!tail submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNYP8zcGLb-O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JT9gFlOpLb-O"
   },
   "source": [
    "Provide your feedback to help me improve further. Thank you."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "tf-keras-mobilevit.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
